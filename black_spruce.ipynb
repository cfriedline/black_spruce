{
 "metadata": {
  "kernelspec": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "display_name": "IPython (Python 2)",
   "language": "python",
   "name": "python2"
  },
  "name": "",
  "signature": "sha256:80edb6bf113590c0d62c05c35ece1641e964e2a49a44cd485296b59acbfefd39"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Bio, os, sys, shutil\n",
      "from Bio import SeqIO, SearchIO\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import font_manager as fm\n",
      "%matplotlib inline\n",
      "from IPython.parallel import Client\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import dill as pickle\n",
      "from Bio.Blast import NCBIXML\n",
      "import pandas as pd\n",
      "from pprint import pprint\n",
      "from lxml import etree\n",
      "from __future__ import division\n",
      "from IPython.display import FileLinks, FileLink\n",
      "import rpy2.robjects as robjects\n",
      "import pandas.rpy.common as com\n",
      "from scipy.stats import gaussian_kdersem\n",
      "from Bio import Entrez\n",
      "import requests, cStringIO\n",
      "import dill\n",
      "import types\n",
      "import sqlite3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn = sqlite3.connect(\"/Users/chris/projects/black_spruce/black_spruce.sqlite\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = robjects.r\n",
      "Entrez.email = \"cfriedline@vcu.edu\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd \"~/projects/black_spruce/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scf_dirs = set()\n",
      "scf_count = 0\n",
      "for root, dirs, files in os.walk(\".\"):\n",
      "    for f in files:\n",
      "        if f.endswith(\".SCF\"):\n",
      "            p = os.path.join(root, f)\n",
      "            p_new = p.replace(\".SCF\", \".scf\")\n",
      "            shutil.move(p, p_new)\n",
      "            scf_dirs.add(os.path.dirname(p_new))\n",
      "            scf_count += 1\n",
      "        elif f.endswith(\".scf\"):\n",
      "            scf_dirs.add(os.path.dirname(os.path.join(root, f)))\n",
      "            scf_count+=1\n",
      "print \"found %d scf files in %d dirs\" % (scf_count, len(scf_dirs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    found 7232 scf files in 4 dirs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scf_dirs\n",
      "seq_dirs = [os.path.abspath(\"%s_seq\" % x) for x in scf_dirs]\n",
      "seq_dirs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in scf_dirs:\n",
      "    sample_id = d[-3:]\n",
      "    files = !ls {d}/*.scf\n",
      "    print sample_id, len(files)\n",
      "    sql = 'insert into sample (sample_id, tissue, raw_reads) values (?,?,?)'\n",
      "    conn.execute(sql, [sample_id, sample_id[-1], len(files)])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = Client()\n",
      "dview = rc[:]\n",
      "lview = rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(dview)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@lview.remote()\n",
      "def run_phred(d, phred_cutoff):\n",
      "    import os\n",
      "    os.environ['PHRED_PARAMETER_FILE'] = '/Users/chris/src/phred-dist-020425.c-acd/phredpar.dat'\n",
      "    r = !~/src/phred-dist-020425.c-acd/phred -id {d.replace(\"_seq\", \"\")} -sd {d} -qd {d} -trim_fasta -trim_alt \"\" -trim_cutoff {phred_cutoff}\n",
      "    return r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phred_cutoff = 0.01\n",
      "phred_res = []\n",
      "for d in seq_dirs:\n",
      "    if not os.path.exists(d):\n",
      "        os.mkdir(d)\n",
      "    phred_res.append(run_phred(d, phred_cutoff))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reads = {}\n",
      "for seq_dir in seq_dirs:\n",
      "    reads[seq_dir] = []\n",
      "    seq_files = !ls {seq_dir} | grep .seq\n",
      "    seq_files = [os.path.join(seq_dir, x) for x in seq_files]\n",
      "    for seq_file in seq_files:\n",
      "        reads[seq_dir].append(SeqIO.read(seq_file, \"fasta\"))\n",
      "print reads.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sample_id(basename):\n",
      "    sample_id = os.path.basename(basename).split(\"_\")\n",
      "    if len(sample_id) == 3:\n",
      "        sample_id = sample_id[1][-3:]\n",
      "    else:\n",
      "        sample_id = sample_id[0][-3:]\n",
      "    return sample_id\n",
      "\n",
      "good_reads = {}\n",
      "total_reads = {}\n",
      "for k, v in reads.items():\n",
      "    good_reads[k] = []\n",
      "    total_reads[k] = 0\n",
      "    for read in v:\n",
      "        if len(read) >= 100:\n",
      "            good_reads[k].append(read)\n",
      "        total_reads[k] += 1\n",
      "print \"good (total) reads in:\"\n",
      "for k, v in good_reads.items():\n",
      "    sample_id = get_sample_id(k)\n",
      "    print \"%s: %d (%d)\" % (os.path.basename(k), len(v), total_reads[k])\n",
      "    sql = 'update sample set phred_reads=? where sample_id=?'\n",
      "    conn.execute(sql, [total_reads[k], sample_id])\n",
      "    sql = 'update sample set length_reads=? where sample_id=?'\n",
      "    conn.execute(sql, [len(v), sample_id])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    good (total) reads in:\n",
      "    im_bscp32N_seq: 1479 (1628)\n",
      "    im_bscp32C_seq: 1551 (1750)\n",
      "    BSCP40C_seq: 1693 (1926)\n",
      "    BSCP40N_seq: 1273 (1849)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_summary(lens):\n",
      "    data = (len(lens), np.mean(lens), np.std(lens), np.min(lens), np.max(lens))\n",
      "    s = \"%d reads, mean(len) = %.2f, sd=%.2f, [%d, %d]\" % data\n",
      "    return s, data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_length_dict(read_dict):\n",
      "    lens = []\n",
      "    len_dict = {}\n",
      "    for k, v in read_dict.items():\n",
      "        len_dict[k] = []\n",
      "        for r in v:\n",
      "            lens.append(len(r))\n",
      "            len_dict[k].append(len(r))\n",
      "    return lens, len_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_hist(read_dict):\n",
      "    lens, len_dict = get_length_dict(read_dict)\n",
      "    plt.hist(lens)\n",
      "    plt.xlabel(\"read length\")\n",
      "    plt.ylabel(\"count\")\n",
      "    title = get_summary(lens)[0]\n",
      "    plt.title(title)\n",
      "    plt.show()\n",
      "    print title\n",
      "    for k, v in len_dict.items():\n",
      "        print os.path.basename(k), get_summary(v)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_hist_ind(read_list):\n",
      "    read_dict = {\"dummy\":read_list}\n",
      "    read_hist(read_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "read_hist(reads)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import phred reads into db\n",
      "lens, len_dict = get_length_dict(reads)\n",
      "get_summary(lens)\n",
      "s = 0\n",
      "for k, v in len_dict.items():\n",
      "    sample_id = get_sample_id(k)\n",
      "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
      "    sql = 'insert into sample_stats values (?,?,?,?,?,?)'\n",
      "    conn.execute(sql, [sample_id, \"phred_reads\", mean, sd, mmin, mmax])\n",
      "    s += total\n",
      "print s\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    7153 reads, mean(len) = 470.17, sd=253.69, [0, 903]\n",
      "    7153 reads, mean(len) = 470.17, sd=253.69, [0, 903]\n",
      "    im_bscp32N_seq 1628 reads, mean(len) = 524.10, sd=229.25, [0, 861]\n",
      "    im_bscp32C_seq 1750 reads, mean(len) = 493.11, sd=227.90, [0, 903]\n",
      "    BSCP40C_seq 1926 reads, mean(len) = 549.79, sd=245.00, [0, 901]\n",
      "    BSCP40N_seq 1849 reads, mean(len) = 318.04, sd=241.26, [0, 815]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "read_hist(good_reads)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    5996 reads, mean(len) = 556.08, sd=175.99, [100, 903]\n",
      "    im_bscp32N_seq 1479 reads, mean(len) = 573.59, sd=176.00, [104, 861]\n",
      "    im_bscp32C_seq 1551 reads, mean(len) = 551.90, sd=167.61, [100, 903]\n",
      "    BSCP40C_seq 1693 reads, mean(len) = 621.89, sd=158.71, [102, 901]\n",
      "    BSCP40N_seq 1273 reads, mean(len) = 453.29, sd=159.60, [100, 815]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import length reads into db (reads passing phred and good length)\n",
      "lens, len_dict = get_length_dict(good_reads)\n",
      "get_summary(lens)\n",
      "s = 0\n",
      "for k, v in len_dict.items():\n",
      "    sample_id = get_sample_id(k)\n",
      "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
      "    sql = 'insert into sample_stats values (?,?,?,?,?,?)'\n",
      "    conn.execute(sql, [sample_id, \"length_reads\", mean, sd, mmin, mmax])\n",
      "    s += total\n",
      "print s\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = {'im_bscp32N_seq':'P32N', \n",
      "         'im_bscp32C_seq':'P32C', \n",
      "         'BSCP40N_seq':'P40N', \n",
      "         'BSCP40C_seq':'P40C'}\n",
      "for k, reads in good_reads.items():\n",
      "    key = os.path.basename(k)\n",
      "    with open(\"%s.fa\" % names[key], \"w\") as out:\n",
      "        SeqIO.write(reads, out, \"fasta\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_files = !grep -c \">\" *.fa | grep -v primer\n",
      "good_files = [os.path.abspath(x.split(':')[0]) for x in good_files]\n",
      "print good_files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Qual:\n",
      "    def __init__(self, name):\n",
      "        self.name = name.split()[0]\n",
      "        self.description = name\n",
      "        self.vals = []\n",
      "        \n",
      "    def __str__(self):\n",
      "        return \">%s\\n%s\" % (self.name, ' '.join(self.vals))\n",
      "    \n",
      "    def add_vals(self, line):\n",
      "        self.vals.append(line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qual_dirs = seq_dirs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qual_dirs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_read_names = set()\n",
      "for k, v in good_reads.items():\n",
      "    for r in v:\n",
      "        good_read_names.add(r.name)\n",
      "for qual_dir in qual_dirs:\n",
      "    print qual_dir\n",
      "    quals = !ls {qual_dir} | grep .qual\n",
      "    quals = [os.path.join(qual_dir, x) for x in quals]\n",
      "    q = None\n",
      "    qual_list = []\n",
      "    for qual in quals:\n",
      "        for line in open(qual):\n",
      "            line = line.strip()\n",
      "            if line.startswith(\">\"):\n",
      "                q = Qual(line[1:])\n",
      "                qual_list.append(q)\n",
      "            else:\n",
      "                q.add_vals(line)\n",
      "    key = os.path.basename(qual_dir)\n",
      "    with open(\"%s.qual\" % names[key], \"w\") as out:\n",
      "        for q in qual_list:\n",
      "            if len(q.vals) > 0:\n",
      "                out.write(\"%s\\n\" % str(q))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Must run seqclean on linux (cdbfasta does not run on Mac).\n",
      "\n",
      "1. Shutdown running mac notebook\n",
      "1. start linux vm\n",
      "1. relaunch notebook from shared folder\n",
      "\n",
      "For example:\n",
      "\n",
      "    chris@vm:~/projects/black_spruce/seqclean$ ~/src/seqclean-x86_64/seqclean P40C.fa -v ~/src/UniVec/UniVec -s ~/projects/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd seqclean/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_clean_files = !ls *.clean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_clean_files = [os.path.abspath(x) for x in seq_clean_files]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_clean_reads = {}\n",
      "for f in seq_clean_files:\n",
      "    seq_clean_reads[f] = []\n",
      "    for read in SeqIO.parse(f, \"fasta\"):\n",
      "        seq_clean_reads[f].append(read)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "read_hist(seq_clean_reads)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "    5630 reads, mean(len) = 445.96, sd=144.39, [100, 842]\n",
      "    P32N.fa.clean 1429 reads, mean(len) = 470.81, sd=159.23, [101, 842]\n",
      "    P32C.fa.clean 1306 reads, mean(len) = 426.42, sd=135.40, [100, 795]\n",
      "    P40C.fa.clean 1652 reads, mean(len) = 473.19, sd=133.18, [102, 825]\n",
      "    P40N.fa.clean 1243 reads, mean(len) = 401.73, sd=135.62, [101, 709]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import seq_clean reads into db (reads passing phred and good length and seq_clean)\n",
      "lens, len_dict = get_length_dict(seq_clean_reads)\n",
      "get_summary(lens)\n",
      "s = 0\n",
      "for k, v in len_dict.items():\n",
      "    sample_id = os.path.basename(k)[1:4]\n",
      "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
      "    sql = 'insert into sample_stats values (?,?,?,?,?,?)'\n",
      "    conn.execute(sql, [sample_id, \"seqclean_reads\", mean, sd, mmin, mmax])\n",
      "    s += total\n",
      "print s\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seqclean_count = !find . -type f | grep '.clean$' | xargs grep -c \">\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for s in seqclean_count:\n",
      "    data = s.split(\":\")\n",
      "    sample_id = os.path.basename(data[0])[1:4]\n",
      "    sql = 'update sample set seqclean_reads=? where sample_id=?'\n",
      "    conn.execute(sql, [int(data[1]), sample_id])\n",
      "    print sample_id, int(data[1])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    ./seqclean/P32C.fa.clean:1306\n",
      "    ./seqclean/P32N.fa.clean:1429\n",
      "    ./seqclean/P40C.fa.clean:1652\n",
      "    ./seqclean/P40N.fa.clean:1243"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Run iAssembler\n",
      "\n",
      "Again, have to use linux b/c not supported on Mac.  `*.clean` files moved to `seqclean`\n",
      " directory for processing\n",
      "\n",
      "For example:\n",
      "\n",
      "    (conda)chris@vm:~/projects/black_spruce/seqclean$ ~/src/iAssembler-v1.3.2.x64/iAssembler.pl -i P32C.fa.clean "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Rename unigene files according to source sample"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/seqclean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_dirs = !ls | grep _output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_dirs = [os.path.abspath(x) for x in output_dirs]\n",
      "output_dirs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import shutil\n",
      "assembled_files = {}\n",
      "for o in output_dirs:\n",
      "    assembled_files[o] = []\n",
      "    key = os.path.basename(o).split(\".\")[0]\n",
      "    for f in os.listdir(o):\n",
      "        if not key in f:\n",
      "            f_name = \"%s_%s\" % (key, f)\n",
      "            shutil.copy(os.path.join(o, f), os.path.join(o, f_name))\n",
      "            assembled_files[o].append(os.path.join(o, f_name))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assembled_fasta = [] \n",
      "for o in output_dirs:\n",
      "    fasta_files = !find $o | grep .fasta\n",
      "    for f in fasta_files:\n",
      "        res = !grep -c \">\" $f\n",
      "        print os.path.basename(os.path.dirname(f)), os.path.basename(f), res[0]\n",
      "        assembled_fasta.append(f)\n",
      "        break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    P32C.fa.clean_output P32C_unigene_seq.fasta 328\n",
      "    P32N.fa.clean_output P32N_unigene_seq.fasta 730\n",
      "    P40C.fa.clean_output P40C_unigene_seq.fasta 434\n",
      "    P40N.fa.clean_output P40N_unigene_seq.fasta 223\n",
      "    all_ests.fa.clean_output unigene_seq.fasta 1945"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigene_reads = {}\n",
      "for f in assembled_fasta:\n",
      "    unigene_reads[f] = []\n",
      "    for read in SeqIO.parse(f, \"fasta\"):\n",
      "        unigene_reads[f].append(read)\n",
      "unigene_reads.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for uni in unigene_reads:\n",
      "    read_hist_ind(unigene_reads[uni])\n",
      "    print os.path.basename(uni)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    223 reads, mean(len) = 480.71, sd=230.99, [104, 1918]\n",
      "    P40N_unigene_seq.fasta\n",
      "    \n",
      "    730 reads, mean(len) = 516.13, sd=186.97, [104, 1377]\n",
      "    P32N_unigene_seq.fasta\n",
      "    \n",
      "    328 reads, mean(len) = 535.98, sd=202.81, [107, 1343]\n",
      "    P32C_unigene_seq.fasta\n",
      "    \n",
      "    1945 reads, mean(len) = 553.77, sd=191.78, [100, 2077]\n",
      "    unigene_seq.fasta\n",
      "    \n",
      "    434 reads, mean(len) = 531.24, sd=187.82, [102, 1748]\n",
      "    P40C_unigene_seq.fasta"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Decorate fasta files with sample (for blast2go)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unigene_files = [\"/Users/chris/projects/black_spruce/seqclean/P32N.fa.clean_output/P32N_unigene_seq.fasta\",\n",
      "\"/Users/chris/projects/black_spruce/seqclean/P32C.fa.clean_output/P32C_unigene_seq.fasta\",\n",
      "\"/Users/chris/projects/black_spruce/seqclean/P40C.fa.clean_output/P40C_unigene_seq.fasta\",\n",
      "\"/Users/chris/projects/black_spruce/seqclean/P40N.fa.clean_output/P40N_unigene_seq.fasta\",\n",
      "\"/Users/chris/projects/black_spruce/seqclean/all_ests.fa.clean_output/all_unigene_seq.fasta\"]\n",
      "for u in unigene_files:\n",
      "    print u\n",
      "    key = os.path.basename(u).split(\"_\")[0]\n",
      "    recs = []\n",
      "    for rec in SeqIO.parse(u, \"fasta\"):\n",
      "        rec.id = \"%s_%s\" % (key, rec.id)\n",
      "        rec.description = \"\"\n",
      "        recs.append(rec)\n",
      "    out_file = \"%s_decorated.fasta\" % u\n",
      "    print out_file\n",
      "    SeqIO.write(recs, open(out_file,\"w\"), \"fasta\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Blast hits (iPlant)\n",
      "\n",
      "    cfriedline@vm64-60:~/projects/black_spruce$ ~/src/ncbi-blast-2.2.29+/bin/blastx -db ~/nr/nr -max_target_seqs 10 -outfmt 5 -num_threads 8 -evalue 1e-5 -query P32C.fa.clean_output/P32C_unigene_seq.fasta -out P32C_blast.xml"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Download blast files from iPlant atmosphere"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!scp atmo:/home/cfriedline/projects/black_spruce/*blast.xml ."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_files = !ls *_blast.xml"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Process blast"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_files = [os.path.abspath(x) for x in blast_files]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_hist(data, title):\n",
      "    plt.hist(data)\n",
      "    plt.title(title)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Get top blast hit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aln_limit = 1\n",
      "hsp_limit = 1\n",
      "for f in blast_files:\n",
      "    query_percs = [] \n",
      "    ident_percs = []\n",
      "    for record in NCBIXML.parse(open(f)):\n",
      "        for i, aln in enumerate(record.alignments):\n",
      "            if i == aln_limit: break\n",
      "            for j, hsp in enumerate(aln.hsps):\n",
      "                if j == hsp_limit: break\n",
      "                query_length = ((hsp.query_end-hsp.query_start)+1.0)\n",
      "                query_perc = query_length/record.query_length\n",
      "                query_percs.append(query_perc)\n",
      "                ident_perc = float(hsp.identities)/hsp.align_length\n",
      "                ident_percs.append(ident_perc)\n",
      "            break\n",
      "    plot_hist(query_percs, \"query percs %s\" % os.path.basename(f))\n",
      "    plot_hist(ident_percs, \"ident percs %s\" % os.path.basename(f))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Filter Blast hits"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_list_chunks(data):\n",
      "    chunk_size = 200 \n",
      "    chunks = [data[i:i+chunk_size] for i in xrange(0,len(data),chunk_size)]\n",
      "    return chunks\n",
      "\n",
      "def create_id_string(id_list):\n",
      "    return '&'.join([\"id=%s\" % x for x in id_list])\n",
      "\n",
      "def get_organism_for_gi(gi_list):\n",
      "    data = {}\n",
      "    search_results = Entrez.read(Entrez.epost(\"protein\", id=\",\".join(gi_list)))\n",
      "    webenv = search_results[\"WebEnv\"]\n",
      "    query_key = search_results[\"QueryKey\"] \n",
      "    handle = Entrez.efetch(db=\"protein\", rettype=\"gb\", webenv=webenv, query_key=query_key)\n",
      "    records = SeqIO.parse(handle, 'genbank')\n",
      "    for i, record in enumerate(records):\n",
      "        assert isinstance(record, Bio.SeqRecord.SeqRecord)\n",
      "        organism = None\n",
      "        for feature in record.features:\n",
      "            if feature.type == \"source\":\n",
      "                organism = feature.qualifiers['organism'][0]\n",
      "        gi = record.annotations['gi']\n",
      "        data[gi] = organism\n",
      "    return data\n",
      "\n",
      "def add_division_to_tax_id(data):\n",
      "    tax_ids = set([v[0] for k,v in data.items()])\n",
      "    search_results = Entrez.read(Entrez.epost(\"taxonomy\", id=\",\".join([x for x in tax_ids if x != \"NOT_FOUND\"])))\n",
      "    webenv = search_results[\"WebEnv\"]\n",
      "    query_key = search_results[\"QueryKey\"] \n",
      "    elems = Entrez.read(Entrez.efetch(db=\"taxonomy\", webenv=webenv, query_key=query_key))\n",
      "    tax_div = {}\n",
      "    for elem in elems:\n",
      "        tax_div[elem['TaxId']] =elem['Division']\n",
      "    for gi, tax_data in data.items():\n",
      "        if tax_data[0] in tax_div:\n",
      "            tax_data.append(tax_div[tax_data[0]])\n",
      "    return data    \n",
      "    \n",
      "def process_elink_xml(xml_list):\n",
      "    res = {}\n",
      "    for chunk in xml_list:\n",
      "        for elem in chunk:\n",
      "            gi = elem['IdList'][0] \n",
      "            tax_id = \"NOT_FOUND\"\n",
      "            try:\n",
      "                tax_id = elem['LinkSetDb'][0]['Link'][0][\"Id\"]\n",
      "            except:\n",
      "                pass\n",
      "            res[gi] = [tax_id]\n",
      "    return add_division_to_tax_id(res)\n",
      "\n",
      "def get_tax_divs_for_gis(id_list):    \n",
      "    dbfrom = \"protein\"\n",
      "    db = \"taxonomy\"\n",
      "    id_chunks = create_list_chunks(id_list)\n",
      "    res = []\n",
      "    for i, chunk in enumerate(id_chunks):\n",
      "        print \"at chunk %d/%d\" % (i, len(id_chunks))\n",
      "        args = \"dbfrom=%s&db=%s&%s\" % (dbfrom,\n",
      "                                       db,\n",
      "                                       create_id_string(chunk))\n",
      "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?%s\" % args\n",
      "        r = requests.post(url)\n",
      "        xml = Entrez.read(cStringIO.StringIO(r.text))\n",
      "        res.append(xml)\n",
      "    return process_elink_xml(res)\n",
      "        \n",
      "def get_divisions_for_hits(blastfile):\n",
      "    gi_set = set()\n",
      "    qresults = (record for record in SearchIO.parse(open(f), \"blast-xml\"))\n",
      "    for qresult in qresults:\n",
      "        for hit in qresult:\n",
      "            gi = hit.id.split(\"|\")[1]\n",
      "            gi_set.add(gi)\n",
      "    id_list = list(gi_set)\n",
      "    gi_tax = get_tax_divs_for_gis(id_list)\n",
      "    return gi_tax\n",
      "\n",
      "divisions = get_divisions_for_hits(\"all_blast.xml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "pickle.dump(divisions, open(\"divisions.pkl\", \"w\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[v for k, v in divisions.items() if k == '213959331']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def blast_hit_filter(hit):\n",
      "    gi = hit.id.split(\"|\")[1]\n",
      "    for hsp in hit.hsps:\n",
      "        query_perc = hsp.query_span/hit.query_len\n",
      "        ident_perc = hsp.ident_num/hsp.aln_span\n",
      "        if query_perc >= hit.min_query_perc:\n",
      "            if ident_perc >= hit.min_ident_perc:\n",
      "                if len(hit.divisions[gi]) == 2 and hit.divisions[gi][1] == hit.division:\n",
      "                    return True\n",
      "            else:\n",
      "                return False\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "def filter_blast_hits(blastfile, min_query_perc, min_ident_perc, show_plot):\n",
      "    from Bio import SearchIO\n",
      "    f = blastfile\n",
      "    key = os.path.basename(f).split(\"_\")[0]\n",
      "    print key, min_query_perc, min_ident_perc\n",
      "    filtered_records = []\n",
      "    bad_records = []\n",
      "    bit_scores = []\n",
      "    e_values = []\n",
      "    query_percs = []\n",
      "    good_query_percs = []\n",
      "    ident_percs = []\n",
      "    good_ident_percs = []\n",
      "    num_filtered = 0\n",
      "    num_total = 0\n",
      "    bad_ident_records = []\n",
      "    bad_query_records = []\n",
      "    division = \"Plants\"\n",
      "    qresults = (record for record in SearchIO.parse(open(f), \"blast-xml\"))\n",
      "    for qresult in qresults:\n",
      "        qresult.id = \"%s_%s\" % (key, qresult.id) #decorate as in decorated unigene fasta (for blast2go)\n",
      "        for hit in qresult.hits:\n",
      "            setattr(hit, \"query_len\", qresult.seq_len)\n",
      "            setattr(hit, \"min_query_perc\", min_query_perc)\n",
      "            setattr(hit, \"min_ident_perc\", min_ident_perc)\n",
      "            setattr(hit, \"divisions\", divisions)\n",
      "            setattr(hit, \"division\", division)\n",
      "        filtered = qresult.hit_filter(blast_hit_filter)\n",
      "        filtered_records.append(filtered)\n",
      "             \n",
      "#     if show_plot:\n",
      "#         plot_hist(bit_scores, \"bit\")\n",
      "#         plot_hist(e_values, \"evalue\")\n",
      "#         plot_hist(query_percs, \"query perc\")\n",
      "#         plot_hist(ident_percs, \"ident perc\")\n",
      "#         plot_hist(good_query_percs, \"good query perc\")\n",
      "#         plot_hist(good_ident_percs, \"good ident perc\")\n",
      "#     print num_filtered, num_total\n",
      "#     #print np.mean(e_values), np.std(e_values), np.min(e_values), np.max(e_values)\n",
      "    out_file = \"%s_filtered_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
      "#     bad_file = \"%s_bad_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
      "#     bad_query_file = \"%s_bad_query_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
      "#     bad_ident_file = \"%s_ident_query_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
      "    try:\n",
      "        SearchIO.write(filtered_records, out_file, \"blast-xml\")\n",
      "#         SearchIO.write(bad_records, bad_file, \"blast-xml\")\n",
      "#         SearchIO.write(bad_query_records, bad_query_file, \"blast-xml\")\n",
      "#         SearchIO.write(bad_ident_records, bad_ident_file, \"blast-xml\")\n",
      "    except:\n",
      "        pass\n",
      "    return filtered_records, num_filtered, num_total\n",
      "\n",
      "x=filter_blast_hits(blast_files[-1], 0, 0, False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=filter_blast_hits(blast_files[-1], 0.5, 0.5, True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filters are (min_query_perc, min_ident_perc)\n",
      "filtered_df = pd.DataFrame(columns=[\"sample\", \"min_query\", \"min_ident\", \"filtered\", \"total\"])\n",
      "\n",
      "blast_filters = [(0,0),\n",
      "           (0.5,0.5),\n",
      "           (0.8,0.8),\n",
      "           (0.5,0),\n",
      "           (0.8,0),\n",
      "           (0,0.5),\n",
      "           (0,0.8),\n",
      "           (0,0.3),\n",
      "           (0.3,0),\n",
      "           (0.3,0.3)]\n",
      "show_plot = False\n",
      "for filt in blast_filters:\n",
      "    for f in blast_files[-1:]: #only process all_blast.xml\n",
      "            res = filter_blast_hits(f, filt[0], filt[1], show_plot)\n",
      "            filtered_df = filtered_df.append({\"sample\":os.path.basename(f), \n",
      "                                              \"min_query\": filt[0], \n",
      "                                              \"min_ident\": filt[1], \n",
      "                                              \"filtered\": res[1], \n",
      "                                              \"total\": res[2]},\n",
      "                                             ignore_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Blast stats for 30/30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_file = \"all_blast.xml_filtered_0.30_query_0.30_ident.xml\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qresults = (record for record in SearchIO.parse(open(blast_file), \"blast-xml\"))\n",
      "assembly_counts = pd.read_csv(\"seqclean/all_ests.fa.clean_output/contig_member.counts\", \n",
      "                              index_col=0,\n",
      "                              sep=\"\\t\")\n",
      "\n",
      "print assembly_counts[0:5]\n",
      "blast_counts = {}\n",
      "for qresult in qresults:\n",
      "    unigene = qresult.id.replace(\"all_\", \"\")\n",
      "    blast_counts[unigene] = len(qresult.hits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_blast_counts(col):\n",
      "    total = 0\n",
      "    for x in col.index:\n",
      "        total += blast_counts[x] * col[x]\n",
      "    return total\n",
      "assembly_counts.apply(get_blast_counts)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    P32C    5038\n",
      "    P40C    5891\n",
      "    P32N    8312\n",
      "    P40N    7017\n",
      "    dtype: int64"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Blast2GO"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "version 2.7.2, jre 1.7.0_65\n",
      "\n",
      "pro server USA1-b2g_may14\n",
      "\n",
      "1. import sequences (decorated unigenes, e.g., PC32_UN001 vs UN001)\n",
      "1. import blast results\n",
      "1. unselect hits without blast\n",
      "1. run mapping step\n",
      "1. run annotation step\n",
      "1. run interpro\n",
      "1. merge interpro\n",
      "1. run annex\n",
      "1. run go-enzymecode\n",
      "1. load kegg maps\n",
      "\n",
      "###Merge\n",
      "1. create new project\n",
      "1. add dats from cambium (or needle)\n",
      "1. redownload kegg maps\n",
      "\n",
      "###Export\n",
      "1. file -> expoort -> generic export - > sequence names for each cambium and needle dat \n",
      "\n",
      "###Exact test\n",
      "1. merge cambium and needle dat\n",
      "1. input test as needle and ref as cambium\n",
      "1. 0.05/FDR exact test\n",
      "1. input test as cambium and ref as needle\n",
      "1. exact test\n",
      "1. export results < 0.05 for all and most specific"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Collapse unigenes by tissue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "decorated = !find . | grep decorated"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "decorated = [os.path.abspath(x) for x in decorated]\n",
      "decorated"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    ['/Users/chris/projects/black_spruce/seqclean/P32C.fa.clean_output/P32C_unigene_seq.fasta_decorated.fasta',\n",
      "     '/Users/chris/projects/black_spruce/seqclean/P32N.fa.clean_output/P32N_unigene_seq.fasta_decorated.fasta',\n",
      "     '/Users/chris/projects/black_spruce/seqclean/P40C.fa.clean_output/P40C_unigene_seq.fasta_decorated.fasta',\n",
      "     '/Users/chris/projects/black_spruce/seqclean/P40N.fa.clean_output/P40N_unigene_seq.fasta_decorated.fasta']"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_dict = {}\n",
      "for d in decorated:\n",
      "    tissue = os.path.basename(d).split(\"_\")[0][-1]\n",
      "    if not tissue in seq_dict:\n",
      "        seq_dict[tissue] = []\n",
      "    for rec in SeqIO.parse(d, \"fasta\"):\n",
      "        seq_dict[tissue].append(rec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "combined_outfiles = []\n",
      "for tissue, seq_list in seq_dict.items():\n",
      "    outfile = \"%s_unigenes_combined.fasta\" % tissue\n",
      "    combined_outfiles.append(outfile)\n",
      "    SeqIO.write(seq_list, outfile, \"fasta\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat C_unigenes_combined.fasta >> all_unigenes_combined.fasta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat N_unigenes_combined.fasta >> all_unigenes_combined.fasta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Collapse all ESTs into a single file\n",
      "(decorated by the source tissue)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_files = !ls seqclean/*.fa"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"insert into sample (sample_id, tissue) values (?,?)\"\n",
      "conn.execute(sql, [\"all\", \"all\"])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_seqs = []\n",
      "seq_lens = {}\n",
      "for e in est_files:\n",
      "    base = os.path.basename(e)\n",
      "    for rec in SeqIO.parse(e, \"fasta\"):\n",
      "        rec.description = rec.description.replace(rec.id, \"\")\n",
      "        rec.id = \"%s_%s\" % (base, rec.id)\n",
      "        est_seqs.append(rec)\n",
      "        seq_lens[rec.id] = len(rec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_lens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SeqIO.write(est_seqs, \"seqclean/all_ests.fa\", \"fasta\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_length_reads = !grep -c \">\" all_ests.fa #this number should equal sum(length_reads) in the db\n",
      "sql = \"update sample set length_reads=? where sample_id=?\"\n",
      "conn.execute(sql, [int(all_length_reads[0]), \"all\"])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Run seqclean on godel for all ESTs\n",
      "\n",
      "runs with blast-2.2.26, after using formatdb -pF on the E. coli genome below\n",
      "\n",
      "    ~/data7/src/seqclean-x86_64/seqclean all_ests.fa -v ~/data7/src/UniVec -s ~/data7/projects/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna\n",
      "    \n",
      "    Collecting cleaning reports\n",
      "\n",
      "    **************************************************\n",
      "    Sequences analyzed:      5996\n",
      "    -----------------------------------\n",
      "                       valid:      5938  (2842 trimmed)\n",
      "                     trashed:        58\n",
      "    **************************************************\n",
      "    ----= Trashing summary =------\n",
      "           by 'NC_010473.fna':       34\n",
      "                    by 'dust':        1\n",
      "                  by 'shortq':       23\n",
      "    ------------------------------\n",
      "    Output file containing only valid and trimmed sequences: all_ests.fa.clean\n",
      "    For trimming and trashing details see cleaning report  : all_ests.fa.cln\n",
      "    --------------------------------------------------\n",
      "    seqclean (all_ests.fa) finished on machine godel97"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_seqclean = !grep -c \">\" all_ests.fa.clean\n",
      "sql = \"update sample set seqclean_reads=? where sample_id=?\"\n",
      "conn.execute(sql, [int(all_seqclean[0]), \"all\"])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Run iAssembler on all ESTs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    ~/data7/src/iAssembler-v1.3.2.x64/iAssembler.pl -i all_ests.fa.clean "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_est_dir = \"~/projects/black_spruce/seqclean/all_ests.fa.clean_output\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd $all_est_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head contig_member"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_est_count_file(contig_member_file):\n",
      "    counts_per_unigene = []\n",
      "    est_counts = 0\n",
      "    unigene_counts = 0\n",
      "    keys = [\"P32C\", \"P40C\", \"P32N\", \"P40N\"]\n",
      "    tissue_dict = {}\n",
      "    with open(\"%s.counts\" % contig_member_file, \"w\") as o:\n",
      "        o.write(\"unigene\\t%s\\n\" % '\\t'.join(keys))\n",
      "        for line in open(contig_member_file):\n",
      "            unigene_counts += 1\n",
      "            counts = {}\n",
      "            for k in keys:\n",
      "                counts[k] = 0\n",
      "            line = line.split()\n",
      "            for elem in line[1:]:\n",
      "                tissue = elem.split(\".\")[0]\n",
      "                if tissue in keys:\n",
      "                    counts[tissue] += 1\n",
      "                    \n",
      "                if not tissue in tissue_dict:\n",
      "                    tissue_dict[tissue] = []\n",
      "                \n",
      "                tissue_dict[tissue].append(elem)\n",
      "                est_counts += 1\n",
      "            vals = []\n",
      "            for k in keys:\n",
      "                vals.append(counts[k])\n",
      "            counts_per_unigene.append(sum(vals))\n",
      "            o.write(\"%s\\t%s\\n\" % (line[0], '\\t'.join([str(x) for x in vals])))\n",
      "    print \"%d ESTs in %d Unigenes\" % (est_counts, unigene_counts)\n",
      "    return counts_per_unigene, tissue_dict\n",
      "\n",
      "counts_per_unigene, tissue_dict = create_est_count_file(\"contig_member\")\n",
      "\n",
      "print \"counts per unigene: mean=%.2f, sd=%.2f, min=%d, max=%d\" % (np.mean(counts_per_unigene),\n",
      "                                                  np.std(counts_per_unigene),\n",
      "                                                  np.min(counts_per_unigene),\n",
      "                                                  np.max(counts_per_unigene))\n",
      "for k, v in tissue_dict.items():\n",
      "    print k, get_summary([seq_lens[x] for x in v])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'insert into unigene (unigene_id, seq, length) values (?,?,?)'\n",
      "for seq in Bio.SeqIO.parse(\"/Users/chris/projects/black_spruce/seqclean/all_ests.fa.clean_output/all_unigene_seq.fasta\", \"fasta\"):\n",
      "    conn.execute(sql, [seq.id, str(seq.seq), len(seq)])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    5938 ESTs in 1945 Unigenes\n",
      "    counts per unigene: mean=3.05, sd=10.21, min=1, max=274\n",
      "    P32N 1475 reads, mean(len) = 574.55, sd=175.23, [104, 861]\n",
      "    P40C 1677 reads, mean(len) = 626.44, sd=152.31, [102, 901]\n",
      "    P40N 1260 reads, mean(len) = 455.70, sd=158.43, [100, 815]\n",
      "    P32C 1526 reads, mean(len) = 557.14, sd=163.35, [100, 903]\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Counts in assembly by tissue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uni_counts = pd.read_csv(\"seqclean/all_ests.fa.clean_output/contig_member.counts\", sep=\"\\t\", index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'insert into unigene_sample (unigene_id, sample_id, assembled_reads) values (?,?,?)'\n",
      "for row in uni_counts.index:\n",
      "    unigene_id = row\n",
      "    for col in uni_counts.columns:\n",
      "        sample_id = col[1:]\n",
      "        conn.execute(sql, [unigene_id, sample_id, uni_counts.ix[row, col]])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "uni_counts.apply(np.sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    P32C    1526\n",
      "    P40C    1677\n",
      "    P32N    1475\n",
      "    P40N    1260\n",
      "    dtype: int64\n",
      "    \n",
      "    also: SELECT sample_id, sum(num_reads) FROM unigene_sample group by sample_id"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Blast unigenes against nr"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    cd /home/cfriedline/data7/projects/black_spruce/\n",
      "\n",
      "    $qsub blast_all_ests.sh\n",
      "    ~/src/ncbi-blast-2.2.29+/bin/blastx -db ~/data7/nr/nr -max_target_seqs 10 -outfmt 5 -num_threads 24 -evalue 1e-5 -query all_ests.fa.clean_output/unigene_seq.fasta -out all_blast.xml\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Count BLASTX hits by tissue for unigenes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_blast = \"all_blast.xml\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_blast_filt = filter_blast_hits(all_blast, 0, 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_counts = pd.read_csv(\"seqclean/all_ests.fa.clean_output/contig_member.counts\", sep=\"\\t\", index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tissue_counts = {}\n",
      "for rec in blast_counts:\n",
      "    tissue_counts[rec] = 0\n",
      "for rec in all_blast_filt:\n",
      "    uni = rec.id.split(\"_\")[1] #like 'all_UN0010'\n",
      "    row = blast_counts.ix[uni]\n",
      "    for col in row.index:\n",
      "        tissue_counts[col] += row[col]\n",
      "print tissue_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    {'P32N': 787, 'P40C': 456, 'P40N': 329, 'P32C': 409}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Massage Blast2GO output for topGO in R\n",
      "This file is generated using file -> export -> export annotations -> annotations by go"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read interpro2go\n",
      "ip2go_file = \"/Users/chris/Downloads/interpro2go\"\n",
      "ip2go_data = {}\n",
      "for i, line in enumerate(open(ip2go_file)):\n",
      "    line = line.strip()\n",
      "    if line.startswith(\"Inter\"):\n",
      "        data = [x.strip() for x in line.split(\">\")]\n",
      "        ipr_data = data[0].split()\n",
      "        ipr_id = ipr_data[0].split(\":\")[1]\n",
      "        ipr_info = \" \".join(ipr_data[1:])\n",
      "        go_data = [x for x in data[1].split(\";\")]\n",
      "        go_id = go_data[-1]\n",
      "        go_info = go_data[0]\n",
      "        \n",
      "        if not ipr_id in ip2go_data:\n",
      "            ip2go_data[ipr_id] = [{\"ipr_id\": ipr_id,\n",
      "                                   \"ipr_info\": ipr_info, \n",
      "                                  \"go_id\": go_id,\n",
      "                                  \"go_info\": go_info}]\n",
      "        else:\n",
      "            ip2go_data[ipr_id].append({\"ipr_id\": ipr_id,\n",
      "                                       \"ipr_info\": ipr_info, \n",
      "                                      \"go_id\": go_id,\n",
      "                                      \"go_info\": go_info})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#anno_file = \"all_annot_GOs_20140722_2254.txt\"\n",
      "#anno_file = \"all_annot_GOslim_20140722_2254.txt\"\n",
      "#anno_file = \"all_filtered_annot_GOs_20140807_1654.txt\"\n",
      "#anno_file = \"all_filtered_plants_annot_GOs_20140912_1122.txt\"\n",
      "anno_file = \"all_filtered_plants_30_30_annot_GOs_20140916_1726.txt\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anno_dict = {}\n",
      "for line in open(anno_file):\n",
      "    data = line.split(\"\\t\")\n",
      "    if not data[0] in anno_dict:\n",
      "        key = \"\\t\".join(data[0:1])\n",
      "        anno_dict[key] = []\n",
      "    anno_dict[key].append(data[2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"%s_topGO.txt\" % anno_file, \"w\") as o:\n",
      "    for k in sorted(anno_dict.keys()):\n",
      "        v = anno_dict[k]\n",
      "        o.write(\"%s\\t%s\\n\" % (k, \",\".join(v)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##topGO files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#anno_file = \"all_annot_GOs_20140722_2254.txt_topGO.txt\"\n",
      "#anno_file = \"all_annot_GOslim_20140722_2254.txt_topGO.txt\"\n",
      "#anno_file = \"all_filtered_annot_GOs_20140807_1654.txt_topGO.txt\"\n",
      "#anno_file = \"all_filtered_plants_annot_GOs_20140912_1122.txt_topGO.txt\"\n",
      "anno_file = \"all_filtered_plants_30_30_annot_GOs_20140916_1726.txt_topGO.txt\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head $anno_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = 'insert into unigene_go values (?,?)'\n",
      "for line in open(anno_file):\n",
      "    line = line.strip().split()\n",
      "    unigene_id = line[0].split(\"_\")[1]\n",
      "    gos = line[1].split(\",\")\n",
      "    for go in gos:\n",
      "        conn.execute(sql, [unigene_id, go])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_per_unigene = []\n",
      "for line in open(anno_file):\n",
      "    line = line.strip().split()\n",
      "    terms = line[1].split(\",\")\n",
      "    go_per_unigene.append(len(terms))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "number_of_unigenes_with_go = len(go_per_unigene)\n",
      "print \"number of unigenes with go annotation=%d\" % number_of_unigenes_with_go\n",
      "print \"go stats: mean=%.2f, sd=%.2f, min=%d, max=%d\" % (np.mean(go_per_unigene),\n",
      "                                                        np.std(go_per_unigene),\n",
      "                                                        np.min(go_per_unigene),\n",
      "                                                        np.max(go_per_unigene))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###GOslim\n",
      "    \n",
      "    number of unigenes with go annotation=1061\n",
      "    go stats: mean=5.59, sd=4.52, min=1, max=29\n",
      "    \n",
      "###GO regular\n",
      "\n",
      "    number of unigenes with go annotation=1061\n",
      "    go stats: mean=5.10, sd=4.66, min=1, max=62\n",
      "    \n",
      "### Go regular (filtered, 0.5, 0.5)\n",
      "    number of unigenes with go annotation=603\n",
      "    go stats: mean=5.45, sd=5.90, min=1, max=58\n",
      "    \n",
      "### Go regular (filtered, only plants, 0.5, 0.5)\n",
      "    number of unigenes with go annotation=526\n",
      "    go stats: mean=3.67, sd=2.14, min=1, max=13\n",
      "    \n",
      "### Go regular (filtered, plants, 0.3, 0.3)\n",
      "    number of unigenes with go annotation=933\n",
      "    go stats: mean=5.62, sd=4.72, min=1, max=44"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Run RSEM against unigene assembly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rsem_dir = \"/Users/chris/src/rsem-1.2.15\"\n",
      "rpr = os.path.join(rsem_dir, \"rsem-prepare-reference\")\n",
      "rce = os.path.join(rsem_dir, \"rsem-calculate-expression\")\n",
      "rpm = os.path.join(rsem_dir, \"rsem-plot-model\")\n",
      "rgdm = os.path.join(rsem_dir, \"rsem-generate-data-matrix\")\n",
      "rre = os.path.join(rsem_dir, \"rsem-run-ebseq\")\n",
      "rcf = os.path.join(rsem_dir, \"rsem-control-fdr\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/seqclean/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!mkdir rsem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd rsem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pwd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rpr_cmd = \"%s %s %s %s %s %s\" % (rpr,\n",
      "                           \"--bowtie2\",\n",
      "                           \"--bowtie2-path /Users/chris/src/bowtie2-2.2.3/\",\n",
      "                           \"~/projects/black_spruce/seqclean/all_ests.fa.clean_output/unigene_seq.fasta\",\n",
      "                           \"--no-polyA\",\n",
      "                           \"black_spruce\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Prepare rsem reference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!$rpr_cmd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Compute expression for cambium and needle"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tissue, suffix in zip([\"cambium\", \"needle\"], [\"C\", \"N\"]):\n",
      "    files = []\n",
      "    parents = [\"P32\", \"P40\"]\n",
      "    for parent in parents:\n",
      "        #use the seqcleaned file\n",
      "        f = os.path.join(\"/Users/chris/projects/black_spruce/seqclean/%s%s.fa.clean\" % (parent, suffix))\n",
      "        files.append(f)\n",
      "        ind = \"%s%s\" % (parent, suffix)\n",
      "        rce_cmd = \"%s %s %s %s %s %s %s %s\" % (rce,\n",
      "                                       \"-p 8\",\n",
      "                                       \"--no-qualities\",\n",
      "                                       \"--bowtie2\",\n",
      "                                       \"--bowtie2-path ~/src/bowtie2-2.2.3\",\n",
      "                                       f,\n",
      "                                       \"black_spruce\",\n",
      "                                       ind)\n",
      "        print rce_cmd\n",
      "        # individual files\n",
      "        !$rce_cmd\n",
      "    \n",
      "    # combined by tissue\n",
      "    rce_cmd = \"%s %s %s %s %s %s %s %s\" % (rce,\n",
      "                                       \"-p 8\",\n",
      "                                       \"--no-qualities\",\n",
      "                                       \"--bowtie2\",\n",
      "                                       \"--bowtie2-path ~/src/bowtie2-2.2.3\",\n",
      "                                       \",\".join(files),\n",
      "                                       \"black_spruce\",\n",
      "                                       tissue)\n",
      "    print rce_cmd\n",
      "    !$rce_cmd\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###plot models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stat_dirs = !ls -d *.stat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for s in stat_dirs:\n",
      "    name = s.split(\".\")[0]\n",
      "    plot = \"%s_plot_file.pdf\" % name\n",
      "    cmd = \"%s %s %s\" % (rpm, name, plot)\n",
      "    print cmd\n",
      "    !$cmd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model P32C P32C_plot_file.pdf\n",
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model P32N P32N_plot_file.pdf\n",
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model P40C P40C_plot_file.pdf\n",
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model P40N P40N_plot_file.pdf\n",
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model cambium cambium_plot_file.pdf\n",
      "    /Users/chris/src/rsem-1.2.15/rsem-plot-model needle needle_plot_file.pdf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_results = !ls P*gene*.results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_results = sorted(gene_results, key=lambda name: name[3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gene_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!$rgdm {\" \".join(gene_results)} > all_counts.matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head all_counts.matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"update unigene_sample set mapped_reads=? where unigene_id=? and sample_id=?\"\n",
      "all_counts = pd.read_csv(\"/Users/chris/projects/black_spruce/seqclean/rsem/all_counts.matrix\", \n",
      "                         sep=\"\\t\",\n",
      "                         index_col=0)\n",
      "for row in all_counts.index:\n",
      "    unigene_id = row\n",
      "    for col in all_counts.columns:\n",
      "        sample_id = col[1:4]\n",
      "        conn.execute(sql, [all_counts.ix[row, col],unigene_id, sample_id])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###find DE genes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###run ebseq in R for diagnostics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " %load_ext rpy2.ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "#borrowed and modified from rsem/ebseq/rsem-for-ebseq-de\n",
      "library(EBSeq)\n",
      "data = as.matrix(read.table(\"all_counts.matrix\", header=T, row.names=1))\n",
      "conditions = as.factor(rep(c(\"C1\",\"C2\"),each=2))\n",
      "print(conditions)\n",
      "sizes = MedianNorm(data)\n",
      "gen=50\n",
      "res = EBTest(Data=data,\n",
      "             Conditions=conditions,\n",
      "             sizeFactors=sizes,\n",
      "             maxround=gen,\n",
      "             Print=T, \n",
      "             Qtrm=1,\n",
      "             QtrmCut=2)\n",
      "PP <- as.data.frame(GetPPMat(res))\n",
      "fc_res <- PostFC(res)\n",
      "results <- cbind(PP, fc_res$PostFC, fc_res$RealFC)\n",
      "colnames(results) <- c(\"PPEE\", \"PPDE\", \"PostFC\", \"RealFC\")\n",
      "results <- results[order(results[,\"PPDE\"], decreasing = TRUE),]\n",
      "write.table(results, file = \"all_ebseq_1_2.txt\", sep = \"\\t\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    [1] C1 C1 C2 C2\n",
      "    Levels: C1 C2\n",
      "    Removing transcripts with 100 th quantile < = 5 \n",
      "    169 transcripts will be tested "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!$rcf all_ebseq.txt 0.05 all_ebseq_fdr.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    There are 35 genes/transcripts reported at FDR = 0.05."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "PlotPostVsRawFC(res,fc_res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "#Check for convergence\n",
      "cutoff = 50\n",
      "print(\"Alpha\")\n",
      "print(res$Alpha[1:cutoff])\n",
      "print(\"Beta\")\n",
      "print(res$Beta[1:cutoff])\n",
      "print(\"P\")\n",
      "print(res$P[1:cutoff])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    [1] \"Alpha\"\n",
      "     [1] 1.036734 1.085241 1.114953 1.133414 1.144429 1.151540 1.156885 1.158999\n",
      "     [9] 1.161182 1.162400 1.163383 1.163771 1.163951 1.164855 1.164409 1.164431\n",
      "    [17] 1.164856 1.164259 1.164882 1.164631 1.165125 1.165552 1.165870 1.165701\n",
      "    [25] 1.165528 1.165808 1.165757 1.165621 1.165823 1.165576 1.165556 1.165835\n",
      "    [33] 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835\n",
      "    [41] 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835 1.165835\n",
      "    [49] 1.165835 1.165835\n",
      "    [1] \"Beta\"\n",
      "     [1] 1.233462 1.288055 1.321025 1.341280 1.353745 1.362035 1.367573 1.370358\n",
      "     [9] 1.371908 1.373840 1.375514 1.375678 1.375911 1.376453 1.375835 1.376093\n",
      "    [17] 1.376763 1.376191 1.376429 1.376852 1.377254 1.377545 1.378261 1.377269\n",
      "    [25] 1.376821 1.377388 1.377218 1.376863 1.377280 1.376728 1.376639 1.377255\n",
      "    [33] 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255\n",
      "    [41] 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255 1.377255\n",
      "    [49] 1.377255 1.377255\n",
      "    [1] \"P\"\n",
      "     [1] 0.6428990 0.7778639 0.8607049 0.9120040 0.9438304 0.9641139 0.9769992\n",
      "     [8] 0.9852561 0.9904332 0.9938689 0.9960493 0.9974741 0.9983682 0.9989496\n",
      "    [15] 0.9993308 0.9995652 0.9997121 0.9998184 0.9998868 0.9999285 0.9999539\n",
      "    [22] 0.9999673 0.9999808 0.9999880 0.9999913 0.9999939 0.9999953 0.9999964\n",
      "    [29] 0.9999979 0.9999980 0.9999992 0.9999995 0.9999995 0.9999995 0.9999995\n",
      "    [36] 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995\n",
      "    [43] 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995 0.9999995\n",
      "    [50] 0.9999995"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anno_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    'all_filtered_plants_30_30_annot_GOs_20140916_1726.txt_topGO.txt'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_topgo_file_names(topgo_file):\n",
      "    \"\"\"\n",
      "        removes all_ from names like all_UN0031 so \n",
      "        they match up in R\n",
      "    \"\"\"\n",
      "    renamed = \"%s_stripped.txt\" % topgo_file\n",
      "    with open(renamed, \"w\") as o:\n",
      "        for line in open(topgo_file):\n",
      "            line = line.split(\"\\t\")\n",
      "            line[0] = line[0].split(\"_\")[1]\n",
      "            o.write(\"\\t\".join(line))\n",
      "\n",
      "    print renamed\n",
      "    return renamed\n",
      "    \n",
      "    \n",
      "stripped_file = strip_topgo_file_names(anno_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    all_filtered_plants_30_30_annot_GOs_20140916_1726.txt_topGO.txt_stripped.txt"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "robjects.globalenv[\"stripped_file\"] = os.path.abspath(stripped_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rpy2.ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "print(stripped_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    [1] \"/Users/chris/Drive/Documents/science/postdoc/papers/black_spruce/all_filtered_plants_30_30_annot_GOs_20140916_1726.txt_topGO.txt_stripped.txt\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(topGO)\n",
      "#rm(list=ls())\n",
      "count_file = \"seqclean/rsem/all_counts.matrix\"\n",
      "counts = read.table(count_file, header=T, row.names=1, sep=\"\\t\")\n",
      "gene_names = rownames(counts)\n",
      "\n",
      "cambium_sums = rowSums(counts[,c(\"P32C.genes.results\",\"P40C.genes.results\")])\n",
      "needle_sums = rowSums(counts[,c(\"P32N.genes.results\",\"P40N.genes.results\")])\n",
      "\n",
      "cambium_interesting = cambium_sums[cambium_sums>1]\n",
      "needle_interesting = needle_sums[needle_sums>1]\n",
      "gene_id_2go  = readMappings(file=stripped_file)\n",
      "#gene_id_2go  = readMappings(file=\"all_annot_GOs_20140722_2254.txt_topGO.txt\")\n",
      "#gene_id_2go = readMappings(file=\"all_annot_GOslim_20140722_2254.txt_topGO.txt\")\n",
      "\n",
      "interesting = list()\n",
      "interesting$cambium = cambium_interesting\n",
      "interesting$needle = needle_interesting\n",
      "godata = list()\n",
      "gentables = list()\n",
      "gentables_bh = list()\n",
      "onts = c(\"BP\",\"CC\", \"MF\")\n",
      "sigs = list()\n",
      "for (i in 1:length(onts)) {\n",
      "    for (j in 1:length(interesting)) {\n",
      "        interest = interesting[[j]]\n",
      "        gene_list <- factor(as.integer(gene_names %in% names(interest)))\n",
      "        names(gene_list) <- gene_names\n",
      "        GOdata = new(\"topGOdata\",\n",
      "                     description=paste(names(interesting)[j], onts[i], sep=\"-\"),\n",
      "                     ontology = onts[i], \n",
      "                     allGenes = gene_list, \n",
      "                     annot = annFUN.gene2GO, \n",
      "                     gene2GO = gene_id_2go,\n",
      "                     nodeSize=5)\n",
      "        print(GOdata)\n",
      "        godata = append(godata, GOdata)\n",
      "        classicFisher = runTest(GOdata, algorithm = \"classic\", statistic = \"fisher\")\n",
      "        weight01Fisher = runTest(GOdata, algorithm = \"weight01\", statistic = \"fisher\")\n",
      "        sigs = append(sigs, classicFisher)\n",
      "        printGraph(GOdata, \n",
      "                   classicFisher, \n",
      "                   firstSigNodes = 5, \n",
      "                   fn.prefix = paste(\"tGO\", \"for\", description(GOdata)), \n",
      "                   #fn.prefix = paste(\"tGOslim\", \"for\", description(GOdata)), \n",
      "                   useInfo = \"all\", \n",
      "                   pdfSW = TRUE)\n",
      "        \n",
      "        gt = GenTable(GOdata, \n",
      "                      classicFisher=classicFisher, \n",
      "                      weight01Fisher=weight01Fisher, \n",
      "                      topNodes=length(classicFisher@score), \n",
      "                      orderBy=\"classicFisher\", numChar=1000)\n",
      "        gentables = append(gentables, list(gt))\n",
      "        gt.bh = gt[which(p.adjust(gt[,\"classicFisher\"],method=\"BH\")<=0.05),]\n",
      "        gentables_bh = append(gentables_bh, list(gt.bh))\n",
      "        write.table(gt, file=paste(description(GOdata), \".txt\", sep=\"\"), row.names=F)\n",
      "        write.table(gt.bh, file=paste(description(GOdata), \"_bh.txt\", sep=\"\"), row.names=F)\n",
      "        }    \n",
      "}\n",
      "save.image(\"topgo.Rdata\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Format DGE, counts, and GO into single data frame"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ~/projects/black_spruce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seq_description_file = \"seq_description_blast2go_export_20140917_1059.txt\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_genes = pd.read_csv(\"seqclean/rsem/all_ebseq_fdr.txt\", sep=\"\\t\", header=0, index_col=0)\n",
      "dge_genes[\"sig\"] = True\n",
      "seq_description_data = {}\n",
      "h = open(seq_description_file)\n",
      "header = h.readline().strip()\n",
      "for line in h:\n",
      "    line = line.strip()\n",
      "    line = line.split(\"\\t\")\n",
      "    name = line[0]\n",
      "    desc = None\n",
      "    if len(line) > 1:\n",
      "        desc = line[1].split(\"|\")[-1]\n",
      "    if not name in seq_description_data:\n",
      "        seq_description_data[name] = []\n",
      "    seq_description_data[name].append(desc)\n",
      "for k in seq_description_data:\n",
      "    if seq_description_data[k][0]:\n",
      "       seq_description_data[k] = \";\".join(seq_description_data[k])\n",
      "seq_descriptions = pd.DataFrame(seq_description_data).T\n",
      "seq_descriptions.index = pd.Index([x.replace(\"all_\", \"\") for x in seq_descriptions.index])\n",
      "seq_descriptions.columns = pd.Index(header.split(\"\\t\")[-1:])\n",
      "all_annot = pd.read_csv(stripped_file, \n",
      "                        sep=\"\\t\", \n",
      "                        index_col=0, \n",
      "                        names=[\"unigene\",\"go\"])\n",
      "\n",
      "count_matrix = pd.read_csv(\"seqclean/rsem/all_counts.matrix\", sep=\"\\t\", index_col=0)\n",
      "c_cols = [\"P32C.genes.results\",\"P40C.genes.results\"]\n",
      "n_cols = [x.replace(\"C.\", \"N.\") for x in c_cols]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot = pd.concat([dge_genes, seq_descriptions, all_annot, count_matrix], axis=1, join=\"outer\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"insert into unigene_blast (unigene_id, desc, count) values (?,?,?)\"\n",
      "for row in dge_annot.index:\n",
      "    desc = dge_annot.ix[row, \"Blast Hit Description (HSP)\"]\n",
      "    if desc:\n",
      "        desc_dict = {}\n",
      "        desc = desc.split(\";\")\n",
      "        for d in desc:\n",
      "            d = d.strip()\n",
      "            if not d in desc_dict:\n",
      "                desc_dict[d] = 0\n",
      "            desc_dict[d] += 1\n",
      "        for k, v in desc_dict.items():\n",
      "            conn.execute(sql, [row, k, v])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"update unigene set dge=?, dge_bh=? where unigene_id=?\"\n",
      "for row in dge_annot.index:\n",
      "    sig = dge_annot.ix[row, \"sig\"]\n",
      "    if pd.notnull(sig):\n",
      "        sig = True\n",
      "    else:\n",
      "        sig = False\n",
      "    conn.execute(sql, [sig, False, row])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run before exporting\n",
      "#dge_annot = dge_annot.fillna(\"\")[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_with_go = dge_annot[[pd.notnull(x) for x in dge_annot.go]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(dge_with_go)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_with_go[dge_with_go.sig==True][0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_go(go_id):\n",
      "    ret = []\n",
      "    if pd.notnull(go_id):\n",
      "        ids = go_id.split(\",\")\n",
      "        term = robjects.r(\"Term\")\n",
      "        ontology = robjects.r(\"Ontology\")\n",
      "        for go in ids:\n",
      "            ret.append((term(go)[0], ontology(go)[0]))\n",
      "    return ret"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_count = 2\n",
      "dge_annot['anno'] = dge_annot['go'].apply(get_go)\n",
      "dge_annot['c_sum'] = dge_annot[c_cols].apply(np.sum, axis=1)\n",
      "dge_annot['n_sum'] = dge_annot[n_cols].apply(np.sum, axis=1)\n",
      "dge_annot['needle_dge'] = (dge_annot.c_sum<dge_annot.n_sum) & (dge_annot.n_sum > min_count)\n",
      "dge_annot['cambium_dge'] = (dge_annot.c_sum>dge_annot.n_sum) & (dge_annot.c_sum > min_count)\n",
      "dge_annot['in_cambium'] = dge_annot.c_sum > min_count\n",
      "dge_annot['in_needle'] = dge_annot.n_sum > min_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_dge_col(row):\n",
      "    if row.cambium_dge and row.needle_dge:\n",
      "        return \"B\"\n",
      "    elif row.cambium_dge:\n",
      "        return \"C\"\n",
      "    elif row.needle_dge:\n",
      "        return \"N\"\n",
      "    \n",
      "def add_in_col(row):\n",
      "    if row.in_cambium and row.in_needle:\n",
      "        return \"B\"\n",
      "    elif row.in_cambium:\n",
      "        return \"C\"\n",
      "    elif row.in_needle:\n",
      "        return \"N\"\n",
      "    \n",
      "def add_tissue_totals(row, tissue):\n",
      "    return row[\"P32%s.genes.results\" % tissue] + row[\"P40%s.genes.results\" % tissue]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot['cambium_total'] = dge_annot.apply(add_tissue_totals, args=\"C\", axis=1)\n",
      "dge_annot['needle_total'] = dge_annot.apply(add_tissue_totals, args=\"N\", axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot[\"Tissue\"] = dge_annot.apply(add_in_col, axis=1)\n",
      "dge_annot[\"DGE\"] = dge_annot.apply(add_dge_col, axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_cols = [\"Blast Hit Description (HSP)\", 'Tissue', 'DGE', \"cambium_total\", \"needle_total\"]\n",
      "dge_annot.ix[dge_annot['sig'] == True,output_cols].to_csv(\"dge_with_go.csv\", sep=\"\\t\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot.ix[dge_annot['sig'] == True,output_cols].to_latex(\"manuscript/dge_table.tex\",\n",
      "                                                            longtable=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.set_option(\"display.max_colwidth\", 100000)\n",
      "dge_annot.ix[dge_annot['sig'] == True,output_cols][0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with_anno = list(output_cols)\n",
      "with_anno.append('anno')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "dge_annot.ix[dge_annot['sig'] == True,with_anno][0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot.ix[dge_annot['sig'] == True,][0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"update unigene set dge_tissue=? where unigene_id=?\"\n",
      "for row in dge_annot.index:\n",
      "    dge = dge_annot.ix[row, \"DGE\"]\n",
      "    if dge:\n",
      "        conn.execute(sql, [dge, row])\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dge_annot.ix['UN0555',\"anno\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"insert into unigene_anno values (?,?,?,?)\"\n",
      "for row in dge_annot.index:\n",
      "    unigene_id = row\n",
      "    anno = dge_annot.ix[row, \"anno\"]\n",
      "    if len(anno) > 0:\n",
      "        found = {}\n",
      "        for a in anno:\n",
      "            if a[0]: #skip empty weird annotations (NA_character_)\n",
      "                if a not in found:\n",
      "                    found[a] = 0\n",
      "                found[a] += 1\n",
      "                data = [unigene_id]\n",
      "                for elem in a:\n",
      "                    data.append(elem)\n",
      "                data.append(found[a])\n",
      "                try:\n",
      "                    conn.execute(sql, data)\n",
      "                except:\n",
      "                    print data\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "sessionInf()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    R version 3.1.1 (2014-07-10)\n",
      "    Platform: x86_64-apple-darwin13.1.0 (64-bit)\n",
      "\n",
      "    locale:\n",
      "    [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n",
      "\n",
      "    attached base packages:\n",
      "     [1] grid      parallel  tools     stats     graphics  grDevices utils    \n",
      "     [8] datasets  methods   base     \n",
      "\n",
      "    other attached packages:\n",
      "     [1] EBSeq_1.4.0          gplots_2.14.1        blockmodeling_0.1.8 \n",
      "     [4] Rgraphviz_2.8.1      topGO_2.16.0         SparseM_1.05        \n",
      "     [7] GO.db_2.14.0         RSQLite_0.11.4       DBI_0.2-7           \n",
      "    [10] AnnotationDbi_1.26.0 GenomeInfoDb_1.0.2   Biobase_2.24.0      \n",
      "    [13] BiocGenerics_0.10.0  graph_1.42.0        \n",
      "\n",
      "    loaded via a namespace (and not attached):\n",
      "     [1] bitops_1.0-6       caTools_1.17       gdata_2.13.3       gtools_3.4.1      \n",
      "     [5] IRanges_1.22.10    KernSmooth_2.23-12 lattice_0.20-29    lme4_1.1-7        \n",
      "     [9] MASS_7.3-33        Matrix_1.1-4       minqa_1.2.3        nlme_3.1-117      \n",
      "    [13] nloptr_1.0.4       Rcpp_0.11.2        splines_3.1.1      stats4_3.1.1     \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def combine_go_files(files, name):\n",
      "    go_dict = {}\n",
      "    for f in files:\n",
      "        ont = f.split(\"-\")[1].split(\".\")[0]\n",
      "        df = None\n",
      "        if not ont in go_dict:\n",
      "            df = pd.DataFrame(columns=[\"Term\", \"Significant\"])\n",
      "            go_dict[ont] = df\n",
      "        else:\n",
      "            df = go_dict[ont]\n",
      "        d = pd.read_csv(f, header=0, index_col=0, sep=\" \")\n",
      "        d = d[d.classicFisher <= 0.05]\n",
      "        for i in d.index:\n",
      "            if not i in df.index:\n",
      "                df.ix[i,] = d.ix[i,]\n",
      "            else:\n",
      "                df.ix[i, \"Significant\"] += d.ix[i, \"Significant\"]\n",
      "    out_files = []\n",
      "    for ont, df in go_dict.items():\n",
      "        out = \"%s-%s.txt\" % (name, ont)\n",
      "        out_files.append(out)\n",
      "        df.to_csv(out, sep=\" \")\n",
      "    return sorted(out_files)\n",
      "\n",
      "go_files = sorted([\"needle-BP.txt\",\n",
      "            \"needle-CC.txt\",\n",
      "            \"needle-MF.txt\",\n",
      "            \"cambium-BP.txt\",\n",
      "            \"cambium-CC.txt\",\n",
      "            \"cambium-MF.txt\"])\n",
      "\n",
      "go_bh_files = sorted([\"needle-BP_bh.txt\",\n",
      "            \"needle-CC_bh.txt\",\n",
      "            \"needle-MF_bh.txt\",\n",
      "            \"cambium-BP_bh.txt\",\n",
      "            \"cambium-CC_bh.txt\",\n",
      "            \"cambium-MF_bh.txt\"])\n",
      "\n",
      "\n",
      "\n",
      "goslim_files = sorted([\"needle-BP_slim.txt\",\n",
      "            \"needle-CC_slim.txt\",\n",
      "            \"needle-MF_slim.txt\",\n",
      "            \"cambium-BP_slim.txt\",\n",
      "            \"cambium-CC_slim.txt\",\n",
      "            \"cambium-MF_slim.txt\"])\n",
      "\n",
      "go_combined = combine_go_files(go_files, \"go_combined\")\n",
      "go_bh_combined = combine_go_files(go_bh_files, \"go_combined\")\n",
      "# goslim_combined = combine_go_files(goslim_files, \"goslim_combined\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create GO dataframe and mark which survive BH correction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_bh_combined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_go_dfs(files):\n",
      "    res = pd.DataFrame()\n",
      "    for f in files:\n",
      "        df = pd.read_csv(f, sep=\" \")\n",
      "        file_data = f.split(\"-\")\n",
      "        tissue = file_data[0]\n",
      "        ont = file_data[1][0:2]\n",
      "        df['Ontology'] = ont\n",
      "        df['Tissue'] = tissue\n",
      "        index = pd.MultiIndex.from_arrays([df['GO.ID'], df.Tissue, df.Ontology])\n",
      "        df.index = index\n",
      "        res = res.append(df)\n",
      "    return res\n",
      "go_df = create_go_dfs(go_files)\n",
      "go_bh_df = create_go_dfs(go_bh_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in go_df.index:\n",
      "    bh = False\n",
      "    if i in go_bh_df.index:\n",
      "        bh = True\n",
      "    go_df.ix[i, \"BH\"] = bh\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_terms = go_df[\"GO.ID\"].unique()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sql = \"insert into go (go_id,tissue,ontology,term,annotated,significant,expected,p,bh) values (?,?,?,?,?,?,?,?,?)\"\n",
      "for row in go_df.index:\n",
      "    go_id, tissue, ontology = row\n",
      "    data = [go_id,\n",
      "            tissue,\n",
      "            ontology,\n",
      "            go_df.ix[row, \"Term\"],\n",
      "            go_df.ix[row, \"Annotated\"],\n",
      "            go_df.ix[row, \"Significant\"],\n",
      "            go_df.ix[row, \"Expected\"],\n",
      "            go_df.ix[row, \"classicFisher\"],\n",
      "            go_df.ix[row, \"BH\"]]\n",
      "    conn.execute(sql, data)\n",
      "conn.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_df[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_counts = {}\n",
      "for term in go_terms:\n",
      "    for i in dge_annot.index:\n",
      "        terms = dge_annot.ix[i, \"go\"]\n",
      "        if pd.notnull(terms) and term in terms:\n",
      "            if not term in go_counts:\n",
      "                go_counts[term] = {\"c\":0, \"n\":0}\n",
      "            go_counts[term][\"c\"] += dge_annot.ix[i, \"c_sum\"]\n",
      "            go_counts[term][\"n\"] += dge_annot.ix[i, \"n_sum\"]\n",
      "\n",
      "go_counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "go_df.Term"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_go_counts(row, tissue):\n",
      "    if row['GO.ID'] in go_counts:\n",
      "        return go_counts[row['GO.ID']][tissue]\n",
      "    return 0\n",
      "\n",
      "for k, v in go_df.groupby([\"Tissue\", \"Ontology\"]):\n",
      "    pd.set_option(\"display.max_colwidth\", 10000)\n",
      "    sig_v = v[v.classicFisher < 0.05]\n",
      "    sig_v[\"Cambium\"] = sig_v.apply(get_go_counts, args=\"c\", axis=1)\n",
      "    sig_v[\"Needle\"] = sig_v.apply(get_go_counts, args=\"n\", axis=1)\n",
      "    sig_v[['Term', 'classicFisher', 'BH', \"Cambium\", \"Needle\"]].to_csv(\"go_%s_counts.csv\" % \n",
      "                                                  '_'.join(k),\n",
      "                                                  sep=\"|\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latex_files = []\n",
      "for k, v in go_df.groupby([\"Tissue\", \"Ontology\"]):\n",
      "    sig_v = v[v.classicFisher < 0.05]\n",
      "    sig_v[['Term', 'classicFisher', 'BH']].to_csv(\"go_%s.txt\" % \n",
      "                                                  '_'.join(k))\n",
      "    lf = \"manuscript/go_%s.tex\" % '_'.join(k)\n",
      "    sig_v[['Term', 'classicFisher', 'BH']].to_latex(lf) \n",
      "    latex_files.append(lf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_caption(file_name):\n",
      "    data = os.path.basename(file_name).split(\"_\")\n",
      "    caption = \"Significant GO categories for %s ontology in the %s tissue. \\\n",
      "BH indicates which of the topGO classic Fisher p-values $< 0.05$ passed correction at \\\n",
      "FDR = 0.05.\"  % (data[-1][0:2],\n",
      "                data[1])\n",
      "    return caption\n",
      "\n",
      "for f in latex_files:\n",
      "    caption = get_caption(f)\n",
      "    label = os.path.basename(f).replace(\"_\", \"-\")[0:-4]\n",
      "    with open(\"%s.tex\" % f.replace(\".tex\", \"_formatted\"), \"w\") as o:\n",
      "        write_midrule = False\n",
      "        header_suffix = None\n",
      "        for line in open(f):\n",
      "            if \"tabular\" in line:\n",
      "                line = line.replace(\"tabular\", \"longtable\")\n",
      "                if \"begin\" in line:\n",
      "                    line = line.replace(\"{ll\", \"{llll\")\n",
      "                    line += \"\\caption{%s}\\\\\\\\\\n\\label{tab:%s}\\\\\\\\\\n\" % (caption,label)\n",
      "            if \"{}\" in line: \n",
      "                header_suffix = line.strip().split(\"&\")\n",
      "                line = \"\"\n",
      "            if \"GO.ID\" in line:\n",
      "                line = line.split(\"&\")\n",
      "                line = ' & '.join(line[0].split())\n",
      "                line = line + ' & '  + ' & '.join([x.strip() for x in header_suffix[1:]]) + \"\\n\"\n",
      "                line = line.replace(\"BH\", \"BH & Cambium & Needle\")\n",
      "                line = line.replace(\"classicFisher\", \"p-value\")\n",
      "                write_midrule = True\n",
      "            if line.startswith(\"GO:\"):\n",
      "                line = line.split(\"&\")\n",
      "                line[0] = ' & '.join(line[0].split())\n",
      "                line[2] = \"%.6f\" % float(line[2])\n",
      "                line = ' & '.join(line)\n",
      "                go_id = line.split(\"&\")[0].strip()\n",
      "                if go_id in go_counts:\n",
      "                    line = line.strip()\n",
      "                    line = line.replace(\"\\\\\", \"\")\n",
      "                    line += \" & \" + str(go_counts[go_id][\"c\"]) \n",
      "                    line += \" & \" + str(go_counts[go_id][\"n\"]) + \" \\\\\\\\ \" + \"\\n\"\n",
      "                else:\n",
      "                    line = line.strip()\n",
      "                    line = line.replace(\"\\\\\", \"\")\n",
      "                    line += \" & 0 & 0 \\\\\\\\\\n\"\n",
      "            if not \"\\midrule\" in line:\n",
      "                o.write(line)\n",
      "            \n",
      "            if write_midrule:\n",
      "                o.write(\"\\midrule\\n\")\n",
      "                write_midrule=False\n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pie_chart(go_files, max_limit, name, layout):\n",
      "    colormaps = {}\n",
      "    for g in go_files:\n",
      "        data = pd.read_csv(g, sep=\" \")\n",
      "        ont = g.split(\"-\")[1].split(\".\")[0]\n",
      "        if not ont in colormaps:\n",
      "            colormaps[ont] = set()\n",
      "        for i, x in enumerate(data.Term):\n",
      "            colormaps[ont].add(x)\n",
      "            \n",
      "    left  = 0.125  # the left side of the subplots of the figure\n",
      "    right = 0.9    # the right side of the subplots of the figure\n",
      "    bottom = 0.1   # the bottom of the subplots of the figure\n",
      "    top = 0.9      # the top of the subplots of the figure\n",
      "    wspace = 0.2   # the amount of width reserved for blank space between subplots\n",
      "    hspace = 0.2   # the amount of height reserved for white space between subplots\n",
      "    \n",
      "    f, axarr = plt.subplots(layout[0], layout[1], sharex=False, sharey=False)\n",
      "    f.tight_layout()\n",
      "    fp = fm.FontProperties()\n",
      "    if layout[1] > 1:\n",
      "        fp.set_size(14)\n",
      "        f.set_size_inches(20,20)\n",
      "        f.subplots_adjust(hspace=0.2, left=0.125, right=0.85, wspace=0.4)\n",
      "    else:\n",
      "        fp.set_size(8)\n",
      "        f.set_size_inches(7,10)\n",
      "        f.subplots_adjust(hspace=0.1, right=0.9)\n",
      "    row = 0\n",
      "    col = 0\n",
      "    for i, g in enumerate(go_files):\n",
      "        data = pd.read_csv(g, sep=\" \").sort(\"Significant\", ascending=False)\n",
      "        if layout[1] > 1:\n",
      "            data = data[data.classicFisher <= 0.05]\n",
      "        limit = max_limit\n",
      "        \n",
      "        if len(data) < limit:\n",
      "            limit = len(data)\n",
      "       \n",
      "        s = g.split(\"-\")\n",
      "        ont = s[1].split(\".\")[0]\n",
      "        title = \"%s %s\" % (s[0].capitalize(), ont)\n",
      "        all_colors = plt.get_cmap('jet')(np.linspace(0, 1.0, len(colormaps[ont])))\n",
      "        onts = sorted(list(colormaps[ont]))\n",
      "        cmap = {}\n",
      "        for i, o in enumerate(onts):\n",
      "            cmap[o] = i\n",
      "        colors = []\n",
      "        for t in data.Term:\n",
      "            colors.append(all_colors[cmap[t]])\n",
      "        ax = None\n",
      "        if layout[1] > 1:\n",
      "            ax = axarr[row, col]\n",
      "        else:\n",
      "            ax = axarr[row]   \n",
      "            title = ont\n",
      "        wedges, texts = ax.pie(data.Significant[0:limit], \n",
      "                                           labels=data.Term[0:limit], \n",
      "                                           colors=colors)\n",
      "        ax.set_title(\"%s (%d)\" % (title, len(data)))\n",
      "        ax.set_aspect(\"equal\")\n",
      "        \n",
      "        plt.setp(texts, fontproperties=fp)\n",
      "        row += 1\n",
      "        if row == layout[0]:\n",
      "            row = 0\n",
      "            col += 1\n",
      "            \n",
      "    plt.savefig(\"/Users/chris/projects/black_spruce/manuscript/figures/go_%s_%d.pdf\" % (name,max_limit), \n",
      "                bbox_layout=\"tight\", \n",
      "                dpi=600)\n",
      "    plt.show()\n",
      "\n",
      "pie_chart(go_files, 20, \"full\", (3,2))\n",
      "#pie_chart(goslim_files, limit, \"slim\", (3,2))\n",
      "pie_chart(go_combined, 20, \"full_combined\", (3,1))\n",
      "#pie_chart(goslim_combined, limit, \"slim_combined\", (3,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pie_chart(go_bh_files, 20, \"full (corrected)\",(3,2))\n",
      "pie_chart(go_bh_combined, 20, \"full combined (corrected)\",(3,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, g in enumerate(go_files):\n",
      "    data = pd.read_csv(g, sep=\" \").sort(\"Significant\", ascending=False)\n",
      "    density = gaussian_kde(data.classicFisher)\n",
      "    cf = density.covariance_factor()\n",
      "    density.covariance_factor = lambda: cf*0.5\n",
      "    density._compute_covariance()\n",
      "    f, ax = plt.subplots()\n",
      "    ax1 = ax.twinx()\n",
      "    xs = np.linspace(0,1,200)\n",
      "    ax1.plot(xs, density(xs), color=\"red\")\n",
      "    ax1.set_ylabel(\"density\")\n",
      "    ax.hist(data.classicFisher, bins= 50)\n",
      "    ax.set_ylabel(\"count\")\n",
      "    print len(data.classicFisher)\n",
      "    plt.title(g)\n",
      "    plt.xlim((0,1))\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Compile BLAST stats"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bp_counts = {}\n",
      "\n",
      "for row in dge_annot.ix[dge_annot.sig == True,with_anno].index:\n",
      "    dge = dge_annot.ix[row].DGE  # C or N\n",
      "    if not dge in bp_counts:\n",
      "        bp_counts[dge] = {}\n",
      "    for a in dge_annot.ix[row].anno:\n",
      "        if a[1] == 'BP':\n",
      "            if not a[0] in bp_counts[dge]:\n",
      "                bp_counts[dge][a[0]] = 1\n",
      "            else:\n",
      "                bp_counts[dge][a[0]] += 1\n",
      "bp_df = pd.DataFrame(bp_counts)\n",
      "sorted_c = bp_df[np.isfinite(bp_df.C)].sort(\"C\", ascending=False)\n",
      "sorted_n = bp_df[np.isfinite(bp_df.N)].sort(\"N\", ascending=False)\n",
      "sorted_bp_df = sorted_c.append(sorted_n).fillna(\"-\")\n",
      "sorted_bp_df.columns = [\"Cambium\", \"Needle\"]\n",
      "sorted_bp_df.index.name = \"Annotation\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted_bp_df.to_csv(\"manuscript/dge_bp.csv\")\n",
      "sorted_bp_df.to_latex(\"manuscript/dge_bp.tex\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Blastn of unigenes against NCBI plant dbEST\n",
      "\n",
      "Performed using the blastn web interface at NCBI, est database, plants (taxid:3193), max 10 targets, expect threshold 0.001, on 9/30/2014"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_blast = \"2PE5FX9F014-Alignment.xml\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est_results = (record for record in SearchIO.parse(open(est_blast), \"blast-xml\"))\n",
      "num_no_hits = 0\n",
      "num_with_hits = 0\n",
      "query_len_perc_min = 0.5\n",
      "query_ident_perc_min = 0.8\n",
      "num_good_query_perc = 0\n",
      "num_good_ident_perc = 0 \n",
      "num_good_both = 0\n",
      "for rec in est_results:\n",
      "    query_len = rec.seq_len\n",
      "    if len(rec.hits) > 0:\n",
      "        num_with_hits += 1\n",
      "        hsp = rec.hits[0].hsps[0] #top hsp for top hit\n",
      "        query_len_perc = hsp.query_span/query_len\n",
      "        if query_len_perc >= query_len_perc_min:\n",
      "            num_good_query_perc += 1\n",
      "        ident_perc = hsp.ident_num/hsp.aln_span\n",
      "        if ident_perc >= query_ident_perc_min:\n",
      "            num_good_ident_perc += 1\n",
      "        if ident_perc >= query_ident_perc_min and query_len_perc >= query_len_perc_min:\n",
      "            num_good_both += 1\n",
      "    else:\n",
      "        num_no_hits += 1\n",
      "print \"%d had hits; no hits for %d unigenes\" % (num_with_hits, num_no_hits)\n",
      "print \"%d had good query perc at %g\" % (num_good_query_perc, query_len_perc_min)\n",
      "print \"%d had good ident perc at %g\" % (num_good_ident_perc, query_ident_perc_min)\n",
      "print \"%d had good query + ident perc\" % (num_good_both)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    1780 had hits; no hits for 165 unigenes\n",
      "    1629 had good query perc at 0.5\n",
      "    1755 had good ident perc at 0.8"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Blastx of unigenes against P. abies genome\n",
      "    ftp://congenie.org/congenie/BLAST/Z4006_Gene_Prediction/Pabies1.0-all-pep.tar.gz"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = Client(profile=\"default\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = rc[:]\n",
      "lview =rc.load_balanced_view()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "@lview.remote()\n",
      "def run_blast_against_abies():\n",
      "    from Bio.Blast.Applications import NcbiblastxCommandline\n",
      "    import os\n",
      "    query = \"seqclean/all_ests.fa.clean_output/all_unigene_seq.fasta_decorated.fasta\"\n",
      "    out = \"p_abies_blastx.xml\"\n",
      "    blastx_cline = NcbiblastxCommandline(query=query, \n",
      "                                     db=\"/Users/chris/projects/Pabies1.0-all-pep.faa\", \n",
      "                                     evalue=0.001, \n",
      "                                     outfmt=5, \n",
      "                                     out=out,\n",
      "                                     cmd = \"/Users/chris/bnfo/ncbi-blast-2.2.30+/bin/blastx\",\n",
      "                                     num_threads = 8,\n",
      "                                     num_alignments = 10)\n",
      "    blastx_cline()\n",
      "    return os.path.abspath(out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blast_res = run_blast_against_abies()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abies_blast = blast_res.r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abies_blast"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abies_blast = '/Users/chris/Drive/Documents/science/postdoc/papers/black_spruce/p_abies_blastx.xml'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abies_qresults = (record for record in SearchIO.parse(open(abies_blast), \"blast-xml\"))\n",
      "query_percs = []\n",
      "ident_percs = []\n",
      "for qresult in abies_qresults:\n",
      "    if len(qresult.hits) > 0:\n",
      "        hit = qresult.hits[0]\n",
      "        hsp = hit.hsps[0]\n",
      "        query_percs.append(hsp.query_span/qresult.seq_len)\n",
      "        ident_percs.append(hsp.ident_num/hsp.aln_span)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(query_percs)\n",
      "plt.show()\n",
      "plt.hist(ident_percs)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abies_qresults = (record for record in SearchIO.parse(open(abies_blast), \"blast-xml\"))\n",
      "abies_good = []  \n",
      "for qresult in abies_qresults:\n",
      "    hits = qresult.hits\n",
      "    remove = set()\n",
      "    for i, hit in enumerate(hits):\n",
      "        for hsp in hit.hsps:\n",
      "            if hsp.ident_num/hsp.aln_span > 0.8 and hsp.query_span / qresult.seq_len > 0.5:\n",
      "                #keep\n",
      "                pass\n",
      "            else:\n",
      "                remove.add(hit.id)\n",
      "        if i > 0:\n",
      "            remove.add(hit.id)\n",
      "    if len(remove) > 0:\n",
      "        for h in remove:\n",
      "            qresult.pop(h)\n",
      "    abies_good.append(qresult)\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SearchIO.write(abies_good, \"abies_good.xml\", \"blast-xml\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "conn.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": ""
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}