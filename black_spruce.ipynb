{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Bio, os, sys, shutil\n",
    "from Bio import SeqIO, SearchIO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager as fm\n",
    "%matplotlib inline\n",
    "from IPython.parallel import Client\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import dill as pickle\n",
    "from Bio.Blast import NCBIXML\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "from __future__ import division\n",
    "from IPython.display import FileLinks, FileLink\n",
    "import rpy2.robjects as robjects\n",
    "import pandas.rpy.common as com\n",
    "from scipy.stats import gaussian_kde\n",
    "from Bio import Entrez\n",
    "import requests, cStringIO\n",
    "import dill\n",
    "import types\n",
    "import sqlite3\n",
    "import dill\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"/home/cfriedline/gpfs/projects/black_spruce/black_spruce.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = robjects.r\n",
    "Entrez.email = \"cfriedline@vcu.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home = \"/gpfs_fs/home/cfriedline/projects/black_spruce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd $home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scf_dirs = set()\n",
    "scf_count = 0\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "    for f in files:\n",
    "        if f.endswith(\".SCF\"):\n",
    "            p = os.path.join(root, f)\n",
    "            p_new = p.replace(\".SCF\", \".scf\")\n",
    "            shutil.move(p, p_new)\n",
    "            scf_dirs.add(os.path.dirname(p_new))\n",
    "            scf_count += 1\n",
    "        elif f.endswith(\".scf\"):\n",
    "            scf_dirs.add(os.path.dirname(os.path.join(root, f)))\n",
    "            scf_count+=1\n",
    "print \"found %d scf files in %d dirs\" % (scf_count, len(scf_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    found 7232 scf files in 4 dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scf_dirs\n",
    "seq_dirs = [os.path.abspath(\"%s_seq\" % x) for x in scf_dirs]\n",
    "seq_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in scf_dirs:\n",
    "    sample_id = d[-3:]\n",
    "    files = !ls {d}/*.scf\n",
    "    print sample_id, len(files)\n",
    "    sql = 'insert into sample (sample_id, tissue, raw_reads) values (?,?,?)'\n",
    "    conn.execute(sql, [sample_id, sample_id[-1], len(files)])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rc = Client()\n",
    "dview = rc[:]\n",
    "lview = rc.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(dview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@lview.remote()\n",
    "def run_phred(d, phred_cutoff):\n",
    "    import os\n",
    "    os.environ['PHRED_PARAMETER_FILE'] = '/Users/chris/src/phred-dist-020425.c-acd/phredpar.dat'\n",
    "    r = !~/src/phred-dist-020425.c-acd/phred -id {d.replace(\"_seq\", \"\")} -sd {d} -qd {d} -trim_fasta -trim_alt \"\" -trim_cutoff {phred_cutoff}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phred_cutoff = 0.01\n",
    "phred_res = []\n",
    "for d in seq_dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "    phred_res.append(run_phred(d, phred_cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reads = {}\n",
    "for seq_dir in seq_dirs:\n",
    "    reads[seq_dir] = []\n",
    "    seq_files = !ls {seq_dir} | grep .seq\n",
    "    seq_files = [os.path.join(seq_dir, x) for x in seq_files]\n",
    "    for seq_file in seq_files:\n",
    "        reads[seq_dir].append(SeqIO.read(seq_file, \"fasta\"))\n",
    "print reads.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sample_id(basename):\n",
    "    sample_id = os.path.basename(basename).split(\"_\")\n",
    "    if len(sample_id) == 3:\n",
    "        sample_id = sample_id[1][-3:]\n",
    "    else:\n",
    "        sample_id = sample_id[0][-3:]\n",
    "    return sample_id\n",
    "\n",
    "good_reads = {}\n",
    "total_reads = {}\n",
    "for k, v in reads.items():\n",
    "    good_reads[k] = []\n",
    "    total_reads[k] = 0\n",
    "    for read in v:\n",
    "        if len(read) >= 100:\n",
    "            good_reads[k].append(read)\n",
    "        total_reads[k] += 1\n",
    "print \"good (total) reads in:\"\n",
    "for k, v in good_reads.items():\n",
    "    sample_id = get_sample_id(k)\n",
    "    print \"%s: %d (%d)\" % (os.path.basename(k), len(v), total_reads[k])\n",
    "    sql = 'update sample set phred_reads=? where sample_id=?'\n",
    "    conn.execute(sql, [total_reads[k], sample_id])\n",
    "    sql = 'update sample set length_reads=? where sample_id=?'\n",
    "    conn.execute(sql, [len(v), sample_id])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    good (total) reads in:\n",
    "    im_bscp32N_seq: 1479 (1628)\n",
    "    im_bscp32C_seq: 1551 (1750)\n",
    "    BSCP40C_seq: 1693 (1926)\n",
    "    BSCP40N_seq: 1273 (1849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_summary(lens):\n",
    "    data = (len(lens), np.mean(lens), np.std(lens), np.min(lens), np.max(lens))\n",
    "    s = \"%d reads, mean(len) = %.2f, sd=%.2f, [%d, %d]\" % data\n",
    "    return s, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_length_dict(read_dict):\n",
    "    lens = []\n",
    "    len_dict = {}\n",
    "    for k, v in read_dict.items():\n",
    "        len_dict[k] = []\n",
    "        for r in v:\n",
    "            lens.append(len(r))\n",
    "            len_dict[k].append(len(r))\n",
    "    return lens, len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_hist(read_dict):\n",
    "    lens, len_dict = get_length_dict(read_dict)\n",
    "    plt.hist(lens)\n",
    "    plt.xlabel(\"read length\")\n",
    "    plt.ylabel(\"count\")\n",
    "    title = get_summary(lens)[0]\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print title\n",
    "    for k, v in len_dict.items():\n",
    "        print os.path.basename(k), get_summary(v)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_hist_ind(read_list):\n",
    "    read_dict = {\"dummy\":read_list}\n",
    "    read_hist(read_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_hist(reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens, len_dict = get_length_dict(reads)\n",
    "get_summary(lens)\n",
    "s = 0\n",
    "for k, v in len_dict.items():\n",
    "    sample_id = get_sample_id(k)\n",
    "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
    "    print sample_id, total, mean, sd, mmin, mmax\n",
    "    s += total\n",
    "print s\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7153 reads, mean(len) = 470.17, sd=253.69, [0, 903]\n",
    "    7153 reads, mean(len) = 470.17, sd=253.69, [0, 903]\n",
    "    im_bscp32N_seq 1628 reads, mean(len) = 524.10, sd=229.25, [0, 861]\n",
    "    im_bscp32C_seq 1750 reads, mean(len) = 493.11, sd=227.90, [0, 903]\n",
    "    BSCP40C_seq 1926 reads, mean(len) = 549.79, sd=245.00, [0, 901]\n",
    "    BSCP40N_seq 1849 reads, mean(len) = 318.04, sd=241.26, [0, 815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_hist(good_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5996 reads, mean(len) = 556.08, sd=175.99, [100, 903]\n",
    "    im_bscp32N_seq 1479 reads, mean(len) = 573.59, sd=176.00, [104, 861]\n",
    "    im_bscp32C_seq 1551 reads, mean(len) = 551.90, sd=167.61, [100, 903]\n",
    "    BSCP40C_seq 1693 reads, mean(len) = 621.89, sd=158.71, [102, 901]\n",
    "    BSCP40N_seq 1273 reads, mean(len) = 453.29, sd=159.60, [100, 815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens, len_dict = get_length_dict(good_reads)\n",
    "get_summary(lens)\n",
    "s = 0\n",
    "for k, v in len_dict.items():\n",
    "    sample_id = get_sample_id(k)\n",
    "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
    "    print sample_id, total, mean, sd, mmin, mmax\n",
    "    sql = 'insert into sample_stats values (?,?,?,?,?,?)'\n",
    "    conn.execute(sql, [sample_id, \"length_reads\", mean, sd, mmin, mmax])\n",
    "    s += total\n",
    "print s\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "40N 1273 453.285938727 159.598494457 100 815\n",
    "40C 1693 621.893089191 158.71016883 102 901\n",
    "32C 1551 551.904577692 167.60567435 100 903\n",
    "32N 1479 573.585530764 176.002648672 104 861\n",
    "5996\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = {'im_bscp32N_seq':'P32N', \n",
    "         'im_bscp32C_seq':'P32C', \n",
    "         'BSCP40N_seq':'P40N', \n",
    "         'BSCP40C_seq':'P40C'}\n",
    "for k, reads in good_reads.items():\n",
    "    key = os.path.basename(k)\n",
    "    with open(\"%s.fa\" % names[key], \"w\") as out:\n",
    "        SeqIO.write(reads, out, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_files = !grep -c \">\" *.fa | grep -v primer\n",
    "good_files = [os.path.abspath(x.split(':')[0]) for x in good_files]\n",
    "print good_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['/gpfs_fs/home/cfriedline/projects/black_spruce/P32C.fa', \n",
    "'/gpfs_fs/home/cfriedline/projects/black_spruce/P32N.fa', \n",
    "'/gpfs_fs/home/cfriedline/projects/black_spruce/P40C.fa', \n",
    "'/gpfs_fs/home/cfriedline/projects/black_spruce/P40N.fa']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qual:\n",
    "    def __init__(self, name):\n",
    "        self.name = name.split()[0]\n",
    "        self.description = name\n",
    "        self.vals = []\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \">%s\\n%s\" % (self.name, ' '.join(self.vals))\n",
    "    \n",
    "    def add_vals(self, line):\n",
    "        self.vals.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qual_dirs = seq_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qual_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['/gpfs_fs/home/cfriedline/projects/black_spruce/im_bscp32C_seq',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/BSCP40C_seq',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/BSCP40N_seq',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/im_bscp32N_seq']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_read_names = set()\n",
    "for k, v in good_reads.items():\n",
    "    for r in v:\n",
    "        good_read_names.add(r.name)\n",
    "for qual_dir in qual_dirs:\n",
    "    print qual_dir\n",
    "    quals = !ls {qual_dir} | grep .qual\n",
    "    quals = [os.path.join(qual_dir, x) for x in quals]\n",
    "    q = None\n",
    "    qual_list = []\n",
    "    for qual in quals:\n",
    "        for line in open(qual):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                q = Qual(line[1:])\n",
    "                qual_list.append(q)\n",
    "            else:\n",
    "                q.add_vals(line)\n",
    "    key = os.path.basename(qual_dir)\n",
    "    with open(\"%s.qual\" % names[key], \"w\") as out:\n",
    "        for q in qual_list:\n",
    "            if len(q.vals) > 0:\n",
    "                out.write(\"%s\\n\" % str(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Must run seqclean on linux (cdbfasta does not run on Mac).\n",
    "\n",
    "1. Shutdown running mac notebook\n",
    "1. start linux vm\n",
    "1. relaunch notebook from shared folder\n",
    "\n",
    "For example:\n",
    "\n",
    "    chris@vm:~/projects/black_spruce/seqclean$ ~/src/seqclean-x86_64/seqclean P40C.fa -v ~/src/UniVec/UniVec -s ~/projects/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ~/gpfs/projects/black_spruce/seqclean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_clean_files = !ls *.clean | grep -v 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_clean_files = [os.path.abspath(x) for x in seq_clean_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_clean_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_clean_reads = {}\n",
    "for f in seq_clean_files:\n",
    "    seq_clean_reads[f] = []\n",
    "    for read in SeqIO.parse(f, \"fasta\"):\n",
    "        seq_clean_reads[f].append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_hist(seq_clean_reads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    5630 reads, mean(len) = 445.96, sd=144.39, [100, 842]\n",
    "    P32N.fa.clean 1429 reads, mean(len) = 470.81, sd=159.23, [101, 842]\n",
    "    P32C.fa.clean 1306 reads, mean(len) = 426.42, sd=135.40, [100, 795]\n",
    "    P40C.fa.clean 1652 reads, mean(len) = 473.19, sd=133.18, [102, 825]\n",
    "    P40N.fa.clean 1243 reads, mean(len) = 401.73, sd=135.62, [101, 709]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens, len_dict = get_length_dict(seq_clean_reads)\n",
    "get_summary(lens)\n",
    "s = 0\n",
    "for k, v in len_dict.items():\n",
    "    sample_id = os.path.basename(k)[1:4]\n",
    "    total, mean, sd, mmin, mmax = get_summary(v)[1]\n",
    "    sql = 'insert into sample_stats values (?,?,?,?,?,?)'\n",
    "    conn.execute(sql, [sample_id, \"seqclean_reads\", mean, sd, mmin, mmax])\n",
    "    s += total\n",
    "print s\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqclean_count = !find . -type f | grep '.clean$' | grep -v 'all' | xargs grep -c \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in seqclean_count:\n",
    "    data = s.split(\":\")\n",
    "    sample_id = os.path.basename(data[0])[1:4]\n",
    "    sql = 'update sample set seqclean_reads=? where sample_id=?'\n",
    "    conn.execute(sql, [int(data[1]), sample_id])\n",
    "    print sample_id, int(data[1])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./seqclean/P32C.fa.clean:1306\n",
    "    ./seqclean/P32N.fa.clean:1429\n",
    "    ./seqclean/P40C.fa.clean:1652\n",
    "    ./seqclean/P40N.fa.clean:1243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Combine needle and cambium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd seqclean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat P32C.fa.clean P40C.fa.clean > cambium.fa.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat P32N.fa.clean P40N.fa.clean > needle.fa.clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run iAssembler\n",
    "\n",
    "Again, have to use linux b/c not supported on Mac.  `*.clean` files moved to `seqclean`\n",
    " directory for processing\n",
    " \n",
    " Also, edited `iAssembler.pl` and pipeline files in `bin` to use `#!/usr/bin/env perl` instead of `/usr/bin/perl -w`\n",
    "\n",
    "For example:\n",
    "\n",
    "    (conda)chris@vm:~/projects/black_spruce/seqclean$ ~/src/iAssembler-v1.3.2.x64/iAssembler.pl -i P32C.fa.clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Rename unigene files according to source sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ~/gpfs/projects/black_spruce/seqclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dirs = !ls | grep _output | grep -v 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dirs = [os.path.abspath(x) for x in output_dirs]\n",
    "output_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembled_files = {}\n",
    "for o in output_dirs:\n",
    "    assembled_files[o] = []\n",
    "    key = os.path.basename(o).split(\".\")[0]\n",
    "    print key\n",
    "    for f in os.listdir(o):\n",
    "        print f\n",
    "        if not key in f:\n",
    "            f_name = \"%s_%s\" % (key, f)\n",
    "            print f_name\n",
    "            shutil.copy(os.path.join(o, f), os.path.join(o, f_name))\n",
    "            assembled_files[o].append(os.path.join(o, f_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembled_fasta = [] \n",
    "for o in output_dirs:\n",
    "    fasta_files = !find $o | grep '.fasta$' | grep -v '^{o}/unigene_seq.fasta' | grep -v 'decorated' | grep -v 'orfs'\n",
    "    for f in fasta_files:\n",
    "        res = !grep -c \">\" $f\n",
    "        print os.path.basename(os.path.dirname(f)), os.path.basename(f), res[0]\n",
    "        assembled_fasta.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    P32C.fa.clean_output P32C_unigene_seq.fasta 328\n",
    "    P32N.fa.clean_output P32N_unigene_seq.fasta 730\n",
    "    P40C.fa.clean_output P40C_unigene_seq.fasta 434\n",
    "    P40N.fa.clean_output P40N_unigene_seq.fasta 223\n",
    "    all_ests.fa.clean_output unigene_seq.fasta 1945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigene_reads = {}\n",
    "for f in assembled_fasta:\n",
    "    unigene_reads[f] = []\n",
    "    for read in SeqIO.parse(f, \"fasta\"):\n",
    "        unigene_reads[f].append(read)\n",
    "unigene_reads.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for uni in unigene_reads:\n",
    "    read_hist_ind(unigene_reads[uni])\n",
    "    print os.path.abspath(uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    223 reads, mean(len) = 480.71, sd=230.99, [104, 1918]\n",
    "    P40N_unigene_seq.fasta\n",
    "    \n",
    "    730 reads, mean(len) = 516.13, sd=186.97, [104, 1377]\n",
    "    P32N_unigene_seq.fasta\n",
    "    \n",
    "    328 reads, mean(len) = 535.98, sd=202.81, [107, 1343]\n",
    "    P32C_unigene_seq.fasta\n",
    "    \n",
    "    1945 reads, mean(len) = 553.77, sd=191.78, [100, 2077]\n",
    "    unigene_seq.fasta\n",
    "    \n",
    "    434 reads, mean(len) = 531.24, sd=187.82, [102, 1748]\n",
    "    P40C_unigene_seq.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Decorate fasta files with sample (for blast2go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigene_files = [\"/home/cfriedline/gpfs/projects/black_spruce/seqclean/P32N.fa.clean_output/P32N_unigene_seq.fasta\",\n",
    "\"/home/cfriedline/gpfs/projects/black_spruce/seqclean/P32C.fa.clean_output/P32C_unigene_seq.fasta\",\n",
    "\"/home/cfriedline/gpfs/projects/black_spruce/seqclean/P40C.fa.clean_output/P40C_unigene_seq.fasta\",\n",
    "\"/home/cfriedline/gpfs/projects/black_spruce/seqclean/P40N.fa.clean_output/P40N_unigene_seq.fasta\",\n",
    "\"/home/cfriedline/gpfs/projects/black_spruce/seqclean/all_ests.fa.clean_output/all_unigene_seq.fasta\"]\n",
    "for u in unigene_files:\n",
    "    print u\n",
    "    key = os.path.basename(u).split(\"_\")[0]\n",
    "    recs = []\n",
    "    for rec in SeqIO.parse(u, \"fasta\"):\n",
    "        rec.id = \"%s_%s\" % (key, rec.id)\n",
    "        rec.description = \"\"\n",
    "        recs.append(rec)\n",
    "    out_file = \"%s_decorated.fasta\" % u\n",
    "    print out_file\n",
    "    SeqIO.write(recs, open(out_file,\"w\"), \"fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Blast hits (iPlant)\n",
    "\n",
    "    cfriedline@vm64-60:~/projects/black_spruce$ ~/src/ncbi-blast-2.2.29+/bin/blastx -db ~/nr/nr -max_target_seqs 10 -outfmt 5 -num_threads 8 -evalue 1e-5 -query P32C.fa.clean_output/P32C_unigene_seq.fasta -out P32C_blast.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Download blast files from iPlant atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ~/gpfs/projects/black_spruce/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!scp atmo:/home/cfriedline/projects/black_spruce/*blast.xml ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_files = !ls *_blast.xml | grep -v 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Process blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_files = [os.path.abspath(x) for x in blast_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['/gpfs_fs/home/cfriedline/projects/black_spruce/P32C_blast.xml',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/P32N_blast.xml',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/P40C_blast.xml',\n",
    " '/gpfs_fs/home/cfriedline/projects/black_spruce/P40N_blast.xml']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_hist(data, title):\n",
    "    plt.hist(data)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get top blast hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aln_limit = 1\n",
    "hsp_limit = 1\n",
    "for f in blast_files:\n",
    "    query_percs = [] \n",
    "    ident_percs = []\n",
    "    for record in NCBIXML.parse(open(f)):\n",
    "        for i, aln in enumerate(record.alignments):\n",
    "            if i == aln_limit: break\n",
    "            for j, hsp in enumerate(aln.hsps):\n",
    "                if j == hsp_limit: break\n",
    "                query_length = ((hsp.query_end-hsp.query_start)+1.0)\n",
    "                query_perc = query_length/record.query_length\n",
    "                query_percs.append(query_perc)\n",
    "                ident_perc = float(hsp.identities)/hsp.align_length\n",
    "                ident_percs.append(ident_perc)\n",
    "            break\n",
    "    plot_hist(query_percs, \"query percs %s\" % os.path.basename(f))\n",
    "    plot_hist(ident_percs, \"ident percs %s\" % os.path.basename(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Blast hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_list_chunks(data):\n",
    "    chunk_size = 200 \n",
    "    chunks = [data[i:i+chunk_size] for i in xrange(0,len(data),chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def create_id_string(id_list):\n",
    "    return '&'.join([\"id=%s\" % x for x in id_list])\n",
    "\n",
    "def get_organism_for_gi(gi_list):\n",
    "    data = {}\n",
    "    search_results = Entrez.read(Entrez.epost(\"protein\", id=\",\".join(gi_list)))\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"] \n",
    "    handle = Entrez.efetch(db=\"protein\", rettype=\"gb\", webenv=webenv, query_key=query_key)\n",
    "    records = SeqIO.parse(handle, 'genbank')\n",
    "    for i, record in enumerate(records):\n",
    "        assert isinstance(record, Bio.SeqRecord.SeqRecord)\n",
    "        organism = None\n",
    "        for feature in record.features:\n",
    "            if feature.type == \"source\":\n",
    "                organism = feature.qualifiers['organism'][0]\n",
    "        gi = record.annotations['gi']\n",
    "        data[gi] = organism\n",
    "    return data\n",
    "\n",
    "def add_division_to_tax_id(data):\n",
    "    tax_ids = set([v[0] for k,v in data.items()])\n",
    "    search_results = Entrez.read(Entrez.epost(\"taxonomy\", id=\",\".join([x for x in tax_ids if x != \"NOT_FOUND\"])))\n",
    "    webenv = search_results[\"WebEnv\"]\n",
    "    query_key = search_results[\"QueryKey\"] \n",
    "    elems = Entrez.read(Entrez.efetch(db=\"taxonomy\", webenv=webenv, query_key=query_key))\n",
    "    tax_div = {}\n",
    "    for elem in elems:\n",
    "        tax_div[elem['TaxId']] =elem['Division']\n",
    "    for gi, tax_data in data.items():\n",
    "        if tax_data[0] in tax_div:\n",
    "            tax_data.append(tax_div[tax_data[0]])\n",
    "    return data    \n",
    "    \n",
    "def process_elink_xml(xml_list):\n",
    "    res = {}\n",
    "    for chunk in xml_list:\n",
    "        for elem in chunk:\n",
    "            gi = elem['IdList'][0] \n",
    "            tax_id = \"NOT_FOUND\"\n",
    "            try:\n",
    "                tax_id = elem['LinkSetDb'][0]['Link'][0][\"Id\"]\n",
    "            except:\n",
    "                pass\n",
    "            res[gi] = [tax_id]\n",
    "    return add_division_to_tax_id(res)\n",
    "\n",
    "def get_tax_divs_for_gis(id_list):    \n",
    "    dbfrom = \"protein\"\n",
    "    db = \"taxonomy\"\n",
    "    id_chunks = create_list_chunks(id_list)\n",
    "    res = []\n",
    "    for i, chunk in enumerate(id_chunks):\n",
    "        print \"at chunk %d/%d\" % (i, len(id_chunks))\n",
    "        args = \"dbfrom=%s&db=%s&%s\" % (dbfrom,\n",
    "                                       db,\n",
    "                                       create_id_string(chunk))\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?%s\" % args\n",
    "        r = requests.post(url)\n",
    "        xml = Entrez.read(cStringIO.StringIO(r.text))\n",
    "        res.append(xml)\n",
    "    return process_elink_xml(res)\n",
    "        \n",
    "def get_divisions_for_hits(blastfile):\n",
    "    gi_set = set()\n",
    "    qresults = (record for record in SearchIO.parse(open(blastfile), \"blast-xml\"))\n",
    "    for qresult in qresults:\n",
    "        for hit in qresult:\n",
    "            gi = hit.id.split(\"|\")[1]\n",
    "            gi_set.add(gi)\n",
    "    id_list = list(gi_set)\n",
    "    gi_tax = get_tax_divs_for_gis(id_list)\n",
    "    return gi_tax\n",
    "\n",
    "division_data = {}\n",
    "for f in blast_files:\n",
    "    divisions = get_divisions_for_hits(f)\n",
    "    division_data[f] = divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "divisions = {}\n",
    "for f, data in division_data.items():\n",
    "    print f\n",
    "    for key, div in data.items():\n",
    "        if key not in divisions:\n",
    "            divisions[key] = div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dill.dump(divisions, open(os.path.join(home, \"divisions.pkl\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, v in divisions.items():\n",
    "    print k, v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def blast_hit_filter(hit):\n",
    "    gi = hit.id.split(\"|\")[1]\n",
    "    for hsp in hit.hsps:\n",
    "        query_perc = hsp.query_span/hit.query_len\n",
    "        ident_perc = hsp.ident_num/hsp.aln_span\n",
    "        if query_perc >= hit.min_query_perc:\n",
    "            if ident_perc >= hit.min_ident_perc:\n",
    "                if len(hit.divisions[gi]) == 2 and hit.divisions[gi][1] == hit.division:\n",
    "                    return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def filter_blast_hits(blastfile, min_query_perc, min_ident_perc, show_plot):\n",
    "    from Bio import SearchIO\n",
    "    f = blastfile\n",
    "    key = os.path.basename(f).split(\"_\")[0]\n",
    "    print key, min_query_perc, min_ident_perc\n",
    "    filtered_records = []\n",
    "    bad_records = []\n",
    "    bit_scores = []\n",
    "    e_values = []\n",
    "    query_percs = []\n",
    "    good_query_percs = []\n",
    "    ident_percs = []\n",
    "    good_ident_percs = []\n",
    "    num_filtered = 0\n",
    "    num_total = 0\n",
    "    bad_ident_records = []\n",
    "    bad_query_records = []\n",
    "    division = \"Plants\"\n",
    "    qresults = (record for record in SearchIO.parse(open(blastfile), \"blast-xml\"))\n",
    "    for qresult in qresults:\n",
    "        qresult.id = \"%s_%s\" % (key, qresult.id) #decorate as in decorated unigene fasta (for blast2go)\n",
    "        for hit in qresult.hits:\n",
    "            setattr(hit, \"query_len\", qresult.seq_len)\n",
    "            setattr(hit, \"min_query_perc\", min_query_perc)\n",
    "            setattr(hit, \"min_ident_perc\", min_ident_perc)\n",
    "            setattr(hit, \"divisions\", divisions)\n",
    "            setattr(hit, \"division\", division)\n",
    "        filtered = qresult.hit_filter(blast_hit_filter)\n",
    "        filtered_records.append(filtered)\n",
    "             \n",
    "#     if show_plot:\n",
    "#         plot_hist(bit_scores, \"bit\")\n",
    "#         plot_hist(e_values, \"evalue\")\n",
    "#         plot_hist(query_percs, \"query perc\")\n",
    "#         plot_hist(ident_percs, \"ident perc\")\n",
    "#         plot_hist(good_query_percs, \"good query perc\")\n",
    "#         plot_hist(good_ident_percs, \"good ident perc\")\n",
    "#     print num_filtered, num_total\n",
    "#     #print np.mean(e_values), np.std(e_values), np.min(e_values), np.max(e_values)\n",
    "    out_file = \"%s_filtered_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
    "#     bad_file = \"%s_bad_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
    "#     bad_query_file = \"%s_bad_query_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
    "#     bad_ident_file = \"%s_ident_query_%.2f_query_%.2f_ident.xml\" % (f, min_query_perc, min_ident_perc)\n",
    "    try:\n",
    "        SearchIO.write(filtered_records, out_file, \"blast-xml\")\n",
    "#         SearchIO.write(bad_records, bad_file, \"blast-xml\")\n",
    "#         SearchIO.write(bad_query_records, bad_query_file, \"blast-xml\")\n",
    "#         SearchIO.write(bad_ident_records, bad_ident_file, \"blast-xml\")\n",
    "    except:\n",
    "        pass\n",
    "    return filtered_records, num_filtered, num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filters are (min_query_perc, min_ident_perc)\n",
    "filtered_df = pd.DataFrame(columns=[\"sample\", \"min_query\", \"min_ident\", \"filtered\", \"total\"])\n",
    "\n",
    "blast_filters = [(0,0),\n",
    "           (0.5,0.5),\n",
    "           (0.8,0.8),\n",
    "           (0.5,0),\n",
    "           (0.8,0),\n",
    "           (0,0.5),\n",
    "           (0,0.8),\n",
    "           (0,0.3),\n",
    "           (0.3,0),\n",
    "           (0.3,0.3)]\n",
    "show_plot = False\n",
    "for filt in blast_filters:\n",
    "    for f in blast_files[0:1]: #only process all_blast.xml\n",
    "            res = filter_blast_hits(f, filt[0], filt[1], show_plot)\n",
    "            filtered_df = filtered_df.append({\"sample\":os.path.basename(f), \n",
    "                                              \"min_query\": filt[0], \n",
    "                                              \"min_ident\": filt[1], \n",
    "                                              \"filtered\": res[1], \n",
    "                                              \"total\": res[2]},\n",
    "                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Blast2GO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 2.7.2, jre 1.7.0_65\n",
    "\n",
    "pro server USA1-b2g_may14\n",
    "\n",
    "1. import sequences (decorated unigenes, e.g., PC32_UN001 vs UN001)\n",
    "1. import blast results\n",
    "1. unselect hits without blast\n",
    "1. run mapping step\n",
    "1. run annotation step\n",
    "1. run interpro\n",
    "1. merge interpro\n",
    "1. run annex\n",
    "1. run go-enzymecode\n",
    "1. load kegg maps\n",
    "\n",
    "###Merge\n",
    "1. create new project\n",
    "1. add dats from cambium (or needle)\n",
    "1. redownload kegg maps\n",
    "\n",
    "###Export\n",
    "1. file -> expoort -> generic export - > sequence names for each cambium and needle dat \n",
    "\n",
    "###Exact test\n",
    "1. merge cambium and needle dat\n",
    "1. input test as needle and ref as cambium\n",
    "1. 0.05/FDR exact test\n",
    "1. input test as cambium and ref as needle\n",
    "1. exact test\n",
    "1. export results < 0.05 for all and most specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Collapse unigenes by tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decorated = !find . | grep decorated | grep -v 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decorated = [os.path.abspath(x) for x in decorated]\n",
    "decorated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['/Users/chris/projects/black_spruce/seqclean/P32C.fa.clean_output/P32C_unigene_seq.fasta_decorated.fasta',\n",
    "'/Users/chris/projects/black_spruce/seqclean/P32N.fa.clean_output/P32N_unigene_seq.fasta_decorated.fasta',\n",
    "'/Users/chris/projects/black_spruce/seqclean/P40C.fa.clean_output/P40C_unigene_seq.fasta_decorated.fasta',\n",
    "'/Users/chris/projects/black_spruce/seqclean/P40N.fa.clean_output/P40N_unigene_seq.fasta_decorated.fasta']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_dict = {}\n",
    "for d in decorated:\n",
    "    tissue = os.path.basename(d).split(\"_\")[0][-1]\n",
    "    if not tissue in seq_dict:\n",
    "        seq_dict[tissue] = []\n",
    "    for rec in SeqIO.parse(d, \"fasta\"):\n",
    "        seq_dict[tissue].append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd $home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_outfiles = []\n",
    "for tissue, seq_list in seq_dict.items():\n",
    "    outfile = \"%s_unigenes_combined.fasta\" % tissue\n",
    "    combined_outfiles.append(outfile)\n",
    "    SeqIO.write(seq_list, outfile, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat C_unigenes_combined.fasta >> all_unigenes_combined.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat N_unigenes_combined.fasta >> all_unigenes_combined.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Collapse all ESTs into a single file\n",
    "(decorated by the source tissue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ~/gpfs/projects/black_spruce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_files = !ls seqclean/*.fa | grep -v 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = \"insert into sample (sample_id, tissue) values (?,?)\"\n",
    "conn.execute(sql, [\"all\", \"all\"])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est_seqs = []\n",
    "seq_lens = {}\n",
    "for e in est_files:\n",
    "    base = os.path.basename(e)\n",
    "    for rec in SeqIO.parse(e, \"fasta\"):\n",
    "        rec.description = rec.description.replace(rec.id, \"\")\n",
    "        rec.id = \"%s_%s\" % (base, rec.id)\n",
    "        est_seqs.append(rec)\n",
    "        seq_lens[rec.id] = len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SeqIO.write(est_seqs, \"seqclean/all_ests.fa\", \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd seqclean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_length_reads = !grep -c \">\" all_ests.fa #this number should equal sum(length_reads) in the db\n",
    "sql = \"update sample set length_reads=? where sample_id=?\"\n",
    "conn.execute(sql, [int(all_length_reads[0]), \"all\"])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_length_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['5996']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run seqclean on godel for all ESTs\n",
    "\n",
    "runs with blast-2.2.26, after using formatdb -pF on the E. coli genome below\n",
    "\n",
    "    ~/data7/src/seqclean-x86_64/seqclean all_ests.fa -v ~/data7/src/UniVec -s ~/data7/projects/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna\n",
    "    \n",
    "    Collecting cleaning reports\n",
    "\n",
    "    **************************************************\n",
    "    Sequences analyzed:      5996\n",
    "    -----------------------------------\n",
    "                       valid:      5938  (2842 trimmed)\n",
    "                     trashed:        58\n",
    "    **************************************************\n",
    "    ----= Trashing summary =------\n",
    "           by 'NC_010473.fna':       34\n",
    "                    by 'dust':        1\n",
    "                  by 'shortq':       23\n",
    "    ------------------------------\n",
    "    Output file containing only valid and trimmed sequences: all_ests.fa.clean\n",
    "    For trimming and trashing details see cleaning report  : all_ests.fa.cln\n",
    "    --------------------------------------------------\n",
    "    seqclean (all_ests.fa) finished on machine godel97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_seqclean = !grep -c \">\" all_ests.fa.clean\n",
    "sql = \"update sample set seqclean_reads=? where sample_id=?\"\n",
    "conn.execute(sql, [int(all_seqclean[0]), \"all\"])\n",
    "conn.commit()\n",
    "all_seqclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "['5938']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run seqclean with vecscreen params\n",
    "\n",
    "    -q -5 -G 3 -E 3 -F \"m D\" -e 700 -Y 1.75e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "~/data7/src/seqclean-x86_64/seqclean all_ests.fa -v ~/data7/src/UniVec -s ~/data7/projects/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run iAssembler on all ESTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ~/data7/src/iAssembler-v1.3.2.x64/iAssembler.pl -i all_ests.fa.clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_est_dir = \"~/gpfs/projects/black_spruce/seqclean/all_ests.fa.clean_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd $all_est_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head contig_member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_est_count_file(contig_member_file):\n",
    "    counts_per_unigene = []\n",
    "    est_counts = 0\n",
    "    unigene_counts = 0\n",
    "    keys = [\"P32C\", \"P40C\", \"P32N\", \"P40N\"]\n",
    "    tissue_dict = {}\n",
    "    with open(\"%s.counts\" % contig_member_file, \"w\") as o:\n",
    "        o.write(\"unigene\\t%s\\n\" % '\\t'.join(keys))\n",
    "        for line in open(contig_member_file):\n",
    "            unigene_counts += 1\n",
    "            counts = {}\n",
    "            for k in keys:\n",
    "                counts[k] = 0\n",
    "            line = line.split()\n",
    "            for elem in line[1:]:\n",
    "                tissue = elem.split(\".\")[0]\n",
    "                if tissue in keys:\n",
    "                    counts[tissue] += 1\n",
    "                    \n",
    "                if not tissue in tissue_dict:\n",
    "                    tissue_dict[tissue] = []\n",
    "                \n",
    "                tissue_dict[tissue].append(elem)\n",
    "                est_counts += 1\n",
    "            vals = []\n",
    "            for k in keys:\n",
    "                vals.append(counts[k])\n",
    "            counts_per_unigene.append(sum(vals))\n",
    "            o.write(\"%s\\t%s\\n\" % (line[0], '\\t'.join([str(x) for x in vals])))\n",
    "    print \"%d ESTs in %d Unigenes\" % (est_counts, unigene_counts)\n",
    "    return counts_per_unigene, tissue_dict\n",
    "\n",
    "counts_per_unigene, tissue_dict = create_est_count_file(\"contig_member\")\n",
    "\n",
    "print \"counts per unigene: mean=%.2f, sd=%.2f, min=%d, max=%d\" % (np.mean(counts_per_unigene),\n",
    "                                                  np.std(counts_per_unigene),\n",
    "                                                  np.min(counts_per_unigene),\n",
    "                                                  np.max(counts_per_unigene))\n",
    "for k, v in tissue_dict.items():\n",
    "    print k, get_summary([seq_lens[x] for x in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = 'insert into unigene (unigene_id, seq, length) values (?,?,?)'\n",
    "for seq in Bio.SeqIO.parse(\"all_unigene_seq.fasta\", \"fasta\"):\n",
    "    conn.execute(sql, [seq.id, str(seq.seq), len(seq)])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5938 ESTs in 1945 Unigenes\n",
    "    counts per unigene: mean=3.05, sd=10.21, min=1, max=274\n",
    "    P32N 1475 reads, mean(len) = 574.55, sd=175.23, [104, 861]\n",
    "    P40C 1677 reads, mean(len) = 626.44, sd=152.31, [102, 901]\n",
    "    P40N 1260 reads, mean(len) = 455.70, sd=158.43, [100, 815]\n",
    "    P32C 1526 reads, mean(len) = 557.14, sd=163.35, [100, 903]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts in assembly by tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd ~/gpfs/projects/black_spruce/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uni_counts = pd.read_csv(\"seqclean/all_ests.fa.clean_output/contig_member.counts\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql = 'insert into unigene_sample (unigene_id, sample_id, assembled_reads) values (?,?,?)'\n",
    "for row in uni_counts.index:\n",
    "    unigene_id = row\n",
    "    for col in uni_counts.columns:\n",
    "        sample_id = col[1:]\n",
    "        conn.execute(sql, [unigene_id, sample_id, uni_counts.ix[row, col]])\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": "0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uni_counts.apply(np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    P32C    1526\n",
    "    P40C    1677\n",
    "    P32N    1475\n",
    "    P40N    1260\n",
    "    dtype: int64\n",
    "    \n",
    "    also: SELECT sample_id, sum(num_reads) FROM unigene_sample group by sample_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blast unigenes against nr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}